# Copyright © 2023-2024 Apple Inc.
# Streaming generation for StreamingLLM

import time
from dataclasses import dataclass
from typing import Dict, Generator, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

from .models.streaming_cache import DualStreamingCache
from .sample_utils import make_sampler
from .streaming_stopping_criteria import (
    EOSStoppingCriteria,
    MaxLengthStoppingCriteria,
    WordBoundaryStoppingCriteria,
)
from .streaming_utils import StreamingState, calculate_wait_words
from .tokenizer_utils import TokenizerWrapper


@dataclass
class GenerationResponse:
    """
    The output of streaming generation.

    Args:
        text (str): The next segment of decoded text. This can be an empty string.
        token (int): The next token.
        logprobs (mx.array): A vector of log probabilities.
        from_draft (bool): Whether the token was generated by the draft model.
        prompt_tokens (int): The number of tokens in the prompt.
        prompt_tps (float): The prompt processing tokens-per-second.
        generation_tokens (int): The number of generated tokens.
        generation_tps (float): The tokens-per-second for generation.
        peak_memory (float): The peak memory used so far in GB.
        finish_reason (str): The reason the response is being sent: "length", "stop" or `None`
        word_complete (bool): True when a word boundary is reached (streaming-specific).
        source_words_read (int): Number of source words processed so far (streaming-specific).
        target_words_generated (int): Number of target words generated (streaming-specific).
    """

    text: str
    token: int
    logprobs: mx.array
    from_draft: bool
    prompt_tokens: int
    prompt_tps: float
    generation_tokens: int
    generation_tps: float
    peak_memory: float
    finish_reason: Optional[str] = None
    word_complete: bool = False
    source_words_read: int = 0
    target_words_generated: int = 0


def stream_generate_streaming(
    model,
    tokenizer,
    source_token_ids: mx.array,
    source_seg_len: List[int],
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens: int = 1024,
    assistant_start_tokens: Optional[mx.array] = None,
    pe_cache_length: int = 0,
    split_mode: str = "word",
    end_token: str = "<|im_end|>",
    temp: float = 0.0,
    top_p: float = 1.0,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: Optional[int] = 20,
    **kwargs,
) -> Generator[Dict, None, None]:
    """
    Generate tokens using streaming policy (wait-k).

    This implements the core streaming generation algorithm from StreamingLLM,
    alternating between reading source tokens and writing target tokens based
    on the wait-k policy.

    Reference: StreamingLLM's _sample_streaming in generation/generate.py:947-1206

    Args:
        model: The streaming model (with DualStreamingCache support)
        tokenizer: Tokenizer instance
        source_token_ids: Source token IDs (1, seq_len)
        source_seg_len: List of token lengths for each source segment/word
        wait_k: Wait-k parameter (wait for k source words before generating)
        max_new_words: Maximum number of target words to generate
        max_tokens: Maximum total tokens to generate
        assistant_start_tokens: Token IDs for assistant message start
        pe_cache_length: Starting position ID for target tokens (default 0)
        split_mode: Segmentation mode ('word' or 'sentence')
        end_token: End instruction token
        temp: Sampling temperature
        top_p: Top-p sampling parameter
        repetition_penalty: Repetition penalty factor
        repetition_context_size: Context size for repetition penalty

    Yields:
        Dictionary with:
        - text: Generated text for current word
        - token_ids: Generated token IDs
        - is_final: Whether this is the final word
        - source_words_read: Number of source words read so far
        - target_words_generated: Number of target words generated
        - mode: Current mode ('read' or 'write')
        - word_complete: True when a word has been completed (write mode), False otherwise

    Algorithm:
        1. Initialize dual caches and state
        2. While not finished:
           IF reading mode:
             - Read next source word chunk
             - Update source cache
             - Switch to writing if wait-k satisfied
           ELSE (writing mode):
             - Generate tokens until word boundary
             - Update target cache
             - Switch to reading if more source available

    Example:
        >>> # Prepare input
        >>> prepared = prepare_streaming_input(
        ...     "Hello world, how are you?",
        ...     tokenizer,
        ...     wait_k=2
        ... )
        >>> # Generate
        >>> for chunk in stream_generate_streaming(
        ...     model,
        ...     tokenizer,
        ...     prepared["source_token_ids"],
        ...     prepared["source_seg_len"],
        ...     wait_k=2
        ... ):
        ...     print(chunk["text"], end="", flush=True)
    """

    # Create sampler
    sampler = make_sampler(
        temp=temp,
        top_p=top_p,
        min_p=kwargs.get("min_p", 0.0),
        top_k=kwargs.get("top_k", 0),
    )

    # Initialize dual caches for all layers
    if hasattr(model, "make_cache"):
        caches = (
            model.make_cache()
        )  # List of caches (may be mixed: MambaCache + DualStreamingCache)
    else:
        num_layers = len(model.layers)
        caches = [DualStreamingCache() for _ in range(num_layers)]

    # Find first DualStreamingCache for tracking target position
    # (needed for hybrid models where first layer might be MambaCache)
    attn_cache = None
    for cache in caches:
        if isinstance(cache, DualStreamingCache):
            attn_cache = cache
            break

    if attn_cache is None:
        # Fallback: if no DualStreamingCache found, assume all are DualStreamingCache
        attn_cache = caches[0]

    # Initialize streaming state
    state = StreamingState(
        source_seg_len=source_seg_len,
        wait_k=wait_k,
        max_target_words=max_new_words,
        pe_cache_length=pe_cache_length,
    )

    # Initialize stopping criteria
    word_boundary_criteria = WordBoundaryStoppingCriteria(
        tokenizer=tokenizer, max_tokens_per_word=5, end_tokens=[end_token]
    )

    max_length_criteria = MaxLengthStoppingCriteria(max_length=max_tokens)

    eos_criteria = EOSStoppingCriteria(eos_token_id=tokenizer.eos_token_id)

    # Track generated tokens
    all_target_tokens = []
    current_word_tokens = []
    total_tokens_generated = 0

    # Get assistant start tokens (if provided)
    if assistant_start_tokens is not None:
        next_input = assistant_start_tokens
    else:
        # Use a simple start token
        # Some tokenizers don't have a BOS token, fall back to EOS or first generated token
        if hasattr(tokenizer, "bos_token_id") and tokenizer.bos_token_id is not None:
            next_input = mx.array([[tokenizer.bos_token_id]])
        elif hasattr(tokenizer, "eos_token_id") and tokenizer.eos_token_id is not None:
            # Use EOS as start token (common pattern for some tokenizers)
            next_input = mx.array([[tokenizer.eos_token_id]])
        else:
            # Fallback: use token ID 0
            next_input = mx.array([[0]])

    # Source position tracking
    source_pos_offset = 0

    # === MAIN STREAMING LOOP ===
    while not state.finished:

        # === READING PHASE ===
        if state.is_reading and state.should_read_next_source():
            # Calculate how many source tokens to read
            tokens_to_read = state.get_source_tokens_to_read()

            # Skip if no tokens to read (e.g., empty segment)
            if tokens_to_read == 0:
                # Mark as read and continue
                state.mark_source_read()
                # Check if we can start writing
                if state.check_wait_k_policy():
                    state.switch_to_writing()
                    current_word_tokens = []
                continue

            # Get source chunk
            source_chunk = source_token_ids[
                :, source_pos_offset : source_pos_offset + tokens_to_read
            ]

            # Safety check: ensure chunk is not empty
            if source_chunk.shape[1] == 0:
                # This shouldn't happen, but handle it gracefully
                print(
                    f"Warning: Empty source_chunk despite tokens_to_read={tokens_to_read}"
                )
                print(f"  source_token_ids.shape: {source_token_ids.shape}")
                print(f"  source_pos_offset: {source_pos_offset}")
                print(f"  Skipping this segment...")
                state.mark_source_read()
                if state.check_wait_k_policy():
                    state.switch_to_writing()
                    current_word_tokens = []
                continue

            # Create position IDs for this chunk
            position_ids = mx.arange(
                source_pos_offset, source_pos_offset + tokens_to_read
            ).reshape(1, -1)

            # Forward pass in reading mode
            _ = model(
                source_chunk, cache=caches, position_ids=position_ids, is_reading=True
            )

            # Update state
            source_pos_offset += tokens_to_read
            state.mark_source_read()

            # Yield read progress
            yield {
                "text": "",
                "token_ids": [],
                "is_final": False,
                "source_words_read": state.source_words_read,
                "target_words_generated": state.target_words_generated,
                "mode": "read",
                "word_complete": False,
            }

            # Check if we can start writing
            if state.check_wait_k_policy():
                state.switch_to_writing()
                # Reset for next word
                current_word_tokens = []

        # === WRITING PHASE ===
        elif not state.is_reading and state.should_write_next_target():
            token_count = 0
            word_finished = False

            # Generate tokens until word boundary
            while not word_finished:

                try:
                    # Create position IDs for target
                    # Create position IDs for target
                    # Use cache offset to track total target tokens, not just current word
                    if attn_cache.target_offset == 0:
                        # First target token - might be multiple if assistant_start_tokens
                        target_pos = pe_cache_length + len(current_word_tokens)
                    else:
                        # Subsequent tokens - use cache offset
                        target_pos = pe_cache_length + attn_cache.target_offset

                    position_ids = mx.array([[target_pos]])

                    # Forward pass in writing mode
                    logits = model(
                        next_input,
                        cache=caches,
                        position_ids=position_ids,
                        is_reading=False,
                    )
                except Exception as e:
                    print(f"[ERROR] Exception in writing phase: {e}")
                    import traceback

                    traceback.print_exc()
                    raise

                # Sample next token
                logits = logits[:, -1, :]

                # Apply repetition penalty if specified
                if repetition_penalty and repetition_penalty != 1.0:
                    if len(all_target_tokens) > 0:
                        context = all_target_tokens[-repetition_context_size:]
                        logits = apply_repetition_penalty(
                            logits, context, repetition_penalty
                        )

                next_token = sampler(logits)
                token_count += 1
                total_tokens_generated += 1

                # Add to current word
                current_word_tokens.append(int(next_token[0]))

                # *** CHECK EOS FIRST (highest priority) ***
                is_eos = eos_criteria(int(next_token[0]))
                if is_eos:
                    if state.should_read_next_source():
                        current_word_tokens.pop()
                        for cache in caches:
                            if isinstance(cache, DualStreamingCache):
                                cache.pop_target()

                        if len(current_word_tokens) > 0:
                            all_target_tokens.extend(current_word_tokens)
                            word_text = tokenizer.decode(current_word_tokens)
                            state.mark_target_written()
                            
                            yield {
                                "text": word_text,
                                "token_ids": current_word_tokens.copy(),
                                "is_final": False,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "write",
                                "word_complete": True,
                            }

                        last_token = mx.array([[current_word_tokens[-1]]]) if len(current_word_tokens) > 0 else assistant_start_tokens
                        next_input = last_token

                        # 4. Switch to reading mode to get more context
                        state.switch_to_reading()
                        current_word_tokens = []
                        break  # Exit write loop, re-enter main loop in read mode
                    else:
                        # EOS detected - stop everything immediately
                        # Remove the EOS token from output (don't show it to user)
                        current_word_tokens.pop()

                        # Add remaining tokens to output
                        if len(current_word_tokens) > 0:
                            all_target_tokens.extend(current_word_tokens)
                            word_text = tokenizer.decode(current_word_tokens)

                            state.mark_target_written()
                            yield {
                                "text": word_text,
                                "token_ids": current_word_tokens.copy(),
                                "is_final": True,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "write",
                                "word_complete": True,
                            }

                        # Stop generation completely
                        state.finished = True
                        break  # Exit the word generation loop

                # Check stopping criteria
                current_word_array = mx.array(current_word_tokens)

                # 1. Check word boundary
                should_stop, remove_last = word_boundary_criteria(
                    current_word_array, token_count
                )

                # 2. Check max length
                if max_length_criteria(total_tokens_generated):
                    should_stop = True
                    state.finished = True

                if should_stop:
                    word_finished = True

                    # Handle token removal (for space detection)
                    if remove_last and len(current_word_tokens) > 1:
                        removed_token = current_word_tokens.pop()
                        # Need to pop from cache too (only for DualStreamingCache)
                        for cache in caches:
                            if isinstance(cache, DualStreamingCache):
                                cache.target_cache.offset -= 1

                    # Add to all tokens
                    all_target_tokens.extend(current_word_tokens)

                    # Decode word
                    word_text = tokenizer.decode(current_word_tokens)

                    # Mark word as generated
                    state.mark_target_written()

                    # Yield generated word
                    yield {
                        "text": word_text,
                        "token_ids": current_word_tokens.copy(),
                        "is_final": state.finished
                        or not state.should_write_next_target(),
                        "source_words_read": state.source_words_read,
                        "target_words_generated": state.target_words_generated,
                        "mode": "write",
                        "word_complete": True,
                    }

                    # Update next_input to last generated token for continuity
                    if len(current_word_tokens) > 0:
                        next_input = mx.array([[current_word_tokens[-1]]])

                    # Check if we should switch back to reading
                    if not state.finished and state.should_read_next_source():
                        state.switch_to_reading()
                        current_word_tokens = []  # Reset for next word
                    elif not state.should_read_next_source():
                        # No more source to read, stay in writing mode
                        current_word_tokens = []
                    else:
                        # Finished
                        state.finished = True

                else:
                    # Update next_input for next iteration
                    next_input = next_token.reshape(1, 1)

        else:
            # No more reading or writing to do
            print(
                f"[FALLTHROUGH] Neither reading nor writing conditions met, finishing"
            )
            print(
                f"  is_reading={state.is_reading}, should_read={state.should_read_next_source()}"
            )
            print(f"  should_write={state.should_write_next_target()}")
            state.finished = True


def apply_repetition_penalty(
    logits: mx.array, context_tokens: List[int], penalty: float
) -> mx.array:
    """
    Apply repetition penalty to logits.

    Args:
        logits: Logits array (batch, vocab_size)
        context_tokens: List of recent token IDs
        penalty: Penalty factor (> 1 reduces repetition)

    Returns:
        Modified logits
    """
    if len(context_tokens) == 0 or penalty == 1.0:
        return logits

    # Apply penalty to tokens in context
    for token_id in set(context_tokens):
        if logits[0, token_id] < 0:
            logits[0, token_id] *= penalty
        else:
            logits[0, token_id] /= penalty

    return logits


def generate_streaming(
    model,
    tokenizer,
    prompt: str,
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens: int = 1024,
    system_prompt: str = "",
    split_mode: str = "word",
    verbose: bool = False,
    **kwargs,
) -> str:
    """
    Convenient wrapper for streaming generation.

    Args:
        model: The streaming model
        tokenizer: Tokenizer instance
        prompt: Input prompt text
        wait_k: Wait-k parameter
        max_new_words: Maximum words to generate
        max_tokens: Maximum tokens to generate
        system_prompt: System prompt
        split_mode: How to split text ('word' or 'sentence')
        verbose: Whether to print progress
        **kwargs: Additional generation parameters

    Returns:
        Generated text as a single string

    Example:
        >>> from mlx_lm.models.qwen2_streaming import Model, ModelArgs
        >>> model = Model(args)
        >>> text = generate_streaming(
        ...     model,
        ...     tokenizer,
        ...     "Translate to French: Hello world",
        ...     wait_k=3
        ... )
    """
    from .streaming_data_utils import prepare_streaming_input

    # Prepare input
    prepared = prepare_streaming_input(
        source_text=prompt,
        tokenizer=tokenizer,
        wait_k=wait_k,
        system_prompt=system_prompt,
        split_mode=split_mode,
        add_space=kwargs.get("add_space", False),
        pe_cache_length=kwargs.get("pe_cache_length", 0),
    )

    if verbose:
        print("=" * 50)
        print(f"Streaming generation with wait-k={wait_k}")
        print("=" * 50)

    # Generate
    generated_text = ""
    for chunk in stream_generate_streaming(
        model=model,
        tokenizer=tokenizer,
        source_token_ids=prepared["source_token_ids"],
        source_seg_len=prepared["source_seg_len"],
        wait_k=wait_k,
        max_new_words=max_new_words,
        max_tokens=max_tokens,
        assistant_start_tokens=prepared["assistant_start_tokens"],
        split_mode=split_mode,
        end_token=prepared["metadata"]["end_token"],
        **kwargs,
    ):
        if chunk["mode"] == "write" and chunk["text"]:
            generated_text += chunk["text"]
            if verbose:
                print(chunk["text"], end="", flush=True)

    if verbose:
        print()
        print("=" * 50)
        print(f"Generated {len(generated_text.split())} words")
        print("=" * 50)

    return generated_text
# Copyright © 2023-2024 Apple Inc.
# Public API for streaming generation

import time
from typing import Generator, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

from .streaming_data_utils import StreamingDataPreparator
from .streaming_generate import GenerationResponse, stream_generate_streaming
from .tokenizer_utils import TokenizerWrapper


def stream_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    prompt: Union[str, mx.array, List[int]],
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens_per_word: int = 50,
    system_prompt: str = "Translate the following English paragraph to French",
    temp: float = 0.0,
    top_p: float = 1.0,
    min_p: float = 0.0,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: int = 20,
    **kwargs,
) -> Generator[GenerationResponse, None, None]:
    """
    A generator producing text using StreamingLLM's wait-k policy.

    This function enables streaming generation where the model processes
    source text incrementally and generates output with a configurable lag
    (wait-k policy). This is useful for simultaneous translation, streaming
    transcription, and real-time text processing.

    Args:
        model (nn.Module): The model to use for generation. Must support streaming
            mode (e.g., Qwen2ModelStreaming).
        tokenizer (PreTrainedTokenizer): The tokenizer.
        prompt (Union[str, mx.array, List[int]]): The input prompt string or
            integer tokens. This is treated as the "source" text in streaming mode.
            NOTE: This should be ONLY the source text to process, not including the
            instruction/task description.
        wait_k (int): Number of source words to wait for before generating each
            target word. Default: ``3``.
        max_new_words (Optional[int]): Maximum number of words to generate. If None,
            generates until end of source. Default: ``None``.
        max_tokens_per_word (int): Maximum tokens per generated word. Default: ``50``.
        system_prompt (str): The system/instruction prompt that describes the task
            (e.g., "Translate the following English paragraph to French"). This is
            kept separate from the source text and NOT segmented word-by-word.
            Default: ``"Translate the following English paragraph to French"``.
        temp (float): Sampling temperature. Default: ``0.0`` (greedy).
        top_p (float): Sampling top-p. Default: ``1.0``.
        min_p (float): Sampling min-p. Default: ``0.0``.
        repetition_penalty (Optional[float]): Repetition penalty factor. Default: ``None``.
        repetition_context_size (int): Context size for repetition penalty. Default: ``20``.
        kwargs: Additional keyword arguments (ignored for compatibility).

    Yields:
        GenerationResponse: An instance containing the generated text segment and
            associated metadata. The response includes:
            - text: Generated text for the current word
            - token: Last token of the word
            - word_complete: True when a word boundary is reached
            - source_words_read: Number of source words processed so far
            - target_words_generated: Number of target words generated so far

    Example:
        >>> from mlx_streaming_llm import load, stream_generate
        >>>
        >>> model, tokenizer = load("Qwen/Qwen2.5-0.5B-Instruct")
        >>> source_text = "Hello, how are you?"
        >>> system_prompt = "Translate the following English paragraph to French"
        >>>
        >>> for response in stream_generate(
        ...     model, tokenizer, source_text,
        ...     wait_k=3,
        ...     system_prompt=system_prompt
        ... ):
        ...     if response.word_complete:
        ...         print(response.text, end=' ', flush=True)
        ...         print(f"[{response.source_words_read} source words]")
    """
    if not isinstance(tokenizer, TokenizerWrapper):
        tokenizer = TokenizerWrapper(tokenizer)

    # Prepare source text with chat template
    preparator = StreamingDataPreparator(tokenizer, system_prompt=system_prompt)

    if isinstance(prompt, str):
        formatted_text, token_ids, seg_lens = preparator.prepare_source_text(prompt)

    elif isinstance(prompt, (list, mx.array)):
        # Already tokenized
        token_ids = prompt.tolist() if isinstance(prompt, mx.array) else prompt
        # Simple word segmentation: assume each token is a word
        seg_lens = [1] * len(token_ids)
    else:
        raise ValueError("Prompt must be a string, list of ints, or mx.array")

    # Track timing
    tic = time.perf_counter()
    prompt_time = 0
    first_token = True
    total_tokens = 0
    source_words_total = len(seg_lens)

    # Convert token_ids to mx.array with batch dimension
    if not isinstance(token_ids, mx.array):
        token_ids = mx.array([token_ids])  # Shape: (1, num_tokens)
    elif token_ids.ndim == 1:
        token_ids = token_ids.reshape(1, -1)

    # Get assistant start tokens from the preparator
    assistant_start_tokens = mx.array([preparator.assistant_tokens])

    # Auto-compute pe_cache_length to avoid position overlap
    # Target tokens must use different position IDs than source tokens
    # Set pe_cache_length = source length to create separate position space
    # Source positions: 0, 1, 2, ..., source_len-1
    # Target positions: source_len, source_len+1, source_len+2, ...
    pe_cache_length = int(token_ids.shape[1])  # Length of source tokens

    # Call our streaming generation function
    for output in stream_generate_streaming(
        model=model,
        tokenizer=tokenizer._tokenizer if hasattr(tokenizer, "_tokenizer") else tokenizer,
        source_token_ids=token_ids,
        source_seg_len=seg_lens,
        wait_k=wait_k,
        max_new_words=max_new_words,
        max_tokens_per_word=max_tokens_per_word,
        assistant_start_tokens=assistant_start_tokens,
        pe_cache_length=pe_cache_length,  # Pass computed value
        temp=temp,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        repetition_context_size=repetition_context_size,
    ):
        if first_token:
            prompt_time = time.perf_counter() - tic
            first_token = False
            tic = time.perf_counter()

        total_tokens += 1
        elapsed = time.perf_counter() - tic

        # Create GenerationResponse compatible output
        response = GenerationResponse(
            text=output.get("text", ""),
            token=output.get("token", 0),
            logprobs=mx.array([0.0]),  # Not provided by streaming generation
            from_draft=False,
            prompt_tokens=len(token_ids),
            prompt_tps=len(token_ids) / prompt_time if prompt_time > 0 else 0.0,
            generation_tokens=total_tokens,
            generation_tps=total_tokens / elapsed if elapsed > 0 else 0.0,
            peak_memory=mx.get_peak_memory() / 1e9,
            finish_reason=None,
        )

        # Add streaming-specific metadata
        response.word_complete = output.get("word_complete", False)
        response.source_words_read = output.get("source_words_read", 0)
        response.target_words_generated = output.get("target_words_generated", 0)

        yield response
