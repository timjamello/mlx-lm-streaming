
import time
from dataclasses import dataclass
from typing import Dict, Generator, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

from .models.streaming_cache import DualStreamingCache
from .sample_utils import make_sampler
from .streaming_stopping_criteria import (
    EOSStoppingCriteria,
    MaxLengthStoppingCriteria,
    WordBoundaryStoppingCriteria,
)
from .streaming_utils import StreamingState, calculate_wait_words
from .tokenizer_utils import TokenizerWrapper


@dataclass
class GenerationResponse:
    """
    The output of streaming generation.

    Args:
        text (str): The next segment of decoded text. This can be an empty string.
        token (int): The next token.
        logprobs (mx.array): A vector of log probabilities.
        from_draft (bool): Whether the token was generated by the draft model.
        prompt_tokens (int): The number of tokens in the prompt.
        prompt_tps (float): The prompt processing tokens-per-second.
        generation_tokens (int): The number of generated tokens.
        generation_tps (float): The tokens-per-second for generation.
        peak_memory (float): The peak memory used so far in GB.
        finish_reason (str): The reason the response is being sent: "length", "stop" or `None`
        word_complete (bool): True when a word boundary is reached (streaming-specific).
        source_words_read (int): Number of source words processed so far (streaming-specific).
        target_words_generated (int): Number of target words generated (streaming-specific).
    """

    text: str
    token: int
    logprobs: mx.array
    from_draft: bool
    prompt_tokens: int
    prompt_tps: float
    generation_tokens: int
    generation_tps: float
    peak_memory: float
    finish_reason: Optional[str] = None
    word_complete: bool = False
    source_words_read: int = 0
    target_words_generated: int = 0


def stream_generate_streaming(
    model,
    tokenizer,
    source_token_ids: mx.array,
    source_seg_len: List[int],
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens: int = 1024,
    assistant_start_tokens: Optional[mx.array] = None,
    pe_cache_length: int = 0,
    split_mode: str = "sentence",
    end_token: str = "<|im_end|>",
    temp: float = 0.0,
    top_p: float = 1.0,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: Optional[int] = 20,
    **kwargs,
) -> Generator[Dict, None, None]:
    """
    Generate tokens using streaming policy (wait-k).

    This implements the core streaming generation algorithm from StreamingLLM,
    alternating between reading source tokens and writing target tokens based
    on the wait-k policy.

    Reference: StreamingLLM's _sample_streaming in generation/generate.py:947-1206

    Args:
        model: The streaming model (with DualStreamingCache support)
        tokenizer: Tokenizer instance
        source_token_ids: Source token IDs (1, seq_len)
        source_seg_len: List of token lengths for each source segment/word
        wait_k: Wait-k parameter (wait for k source words before generating)
        max_new_words: Maximum number of target words to generate
        max_tokens: Maximum total tokens to generate
        assistant_start_tokens: Token IDs for assistant message start
        pe_cache_length: Starting position ID for target tokens (default 0)
        split_mode: Segmentation mode ('word' or 'sentence')
        end_token: End instruction token
        temp: Sampling temperature
        top_p: Top-p sampling parameter
        repetition_penalty: Repetition penalty factor
        repetition_context_size: Context size for repetition penalty

    Yields:
        Dictionary with:
        - text: Generated text for current word
        - token_ids: Generated token IDs
        - is_final: Whether this is the final word
        - source_words_read: Number of source words read so far
        - target_words_generated: Number of target words generated
        - mode: Current mode ('read' or 'write')
        - word_complete: True when a word has been completed (write mode), False otherwise

    Algorithm:
        1. Initialize dual caches and state
        2. While not finished:
           IF reading mode:
             - Read next source word chunk
             - Update source cache
             - Switch to writing if wait-k satisfied
           ELSE (writing mode):
             - Generate tokens until word boundary
             - Update target cache
             - Switch to reading if more source available

    Example:
        >>>
        >>> prepared = prepare_streaming_input(
        ...     "Hello world, how are you?",
        ...     tokenizer,
        ...     wait_k=2
        ... )
        >>>
        >>> for chunk in stream_generate_streaming(
        ...     model,
        ...     tokenizer,
        ...     prepared["source_token_ids"],
        ...     prepared["source_seg_len"],
        ...     wait_k=2
        ... ):
        ...     print(chunk["text"], end="", flush=True)
    """

    sampler = make_sampler(
        temp=temp,
        top_p=top_p,
        min_p=kwargs.get("min_p", 0.0),
        top_k=kwargs.get("top_k", 0),
    )

    if hasattr(model, "make_cache"):
        caches = (
            model.make_cache()
        )
    else:
        num_layers = len(model.layers)
        caches = [DualStreamingCache() for _ in range(num_layers)]

    attn_cache = None
    for cache in caches:
        if isinstance(cache, DualStreamingCache):
            attn_cache = cache
            break

    if attn_cache is None:
        attn_cache = caches[0]

    state = StreamingState(
        source_seg_len=source_seg_len,
        wait_k=wait_k,
        max_target_words=max_new_words,
        pe_cache_length=pe_cache_length,
    )

    word_boundary_criteria = WordBoundaryStoppingCriteria(
        tokenizer=tokenizer, max_tokens_per_word=5, end_tokens=[end_token]
    )

    max_length_criteria = MaxLengthStoppingCriteria(max_length=max_tokens)

    eos_criteria = EOSStoppingCriteria(eos_token_id=tokenizer.eos_token_id)

    all_target_tokens = []
    current_word_tokens = []
    total_tokens_generated = 0
    mask_eos_next_token = False

    if assistant_start_tokens is not None:
        next_input = assistant_start_tokens
    else:
        if hasattr(tokenizer, "bos_token_id") and tokenizer.bos_token_id is not None:
            next_input = mx.array([[tokenizer.bos_token_id]])
        elif hasattr(tokenizer, "eos_token_id") and tokenizer.eos_token_id is not None:
            next_input = mx.array([[tokenizer.eos_token_id]])
        else:
            next_input = mx.array([[0]])

    source_pos_offset = 0

    while not state.finished:

        if state.is_reading and state.should_read_next_source():
            tokens_to_read = state.get_source_tokens_to_read()

            if tokens_to_read == 0:
                state.mark_source_read()
                if state.check_wait_k_policy():
                    state.switch_to_writing()
                    current_word_tokens = []
                continue

            source_chunk = source_token_ids[
                :, source_pos_offset : source_pos_offset + tokens_to_read
            ]

            if source_chunk.shape[1] == 0:
                state.mark_source_read()
                if state.check_wait_k_policy():
                    state.switch_to_writing()
                    current_word_tokens = []
                continue

            position_ids = mx.arange(
                source_pos_offset, source_pos_offset + tokens_to_read
            ).reshape(1, -1)

            _ = model(
                source_chunk, cache=caches, position_ids=position_ids, is_reading=True
            )

            source_pos_offset += tokens_to_read
            state.mark_source_read()

            yield {
                "text": "",
                "token_ids": [],
                "is_final": False,
                "source_words_read": state.source_words_read,
                "target_words_generated": state.target_words_generated,
                "mode": "read",
                "word_complete": False,
            }

            if state.check_wait_k_policy():
                state.switch_to_writing()
                current_word_tokens = []

        elif not state.is_reading and state.should_write_next_target():
            token_count = 0
            word_finished = False

            while not word_finished:

                try:
                    if attn_cache.target_offset == 0:
                        target_pos = pe_cache_length + len(current_word_tokens)
                    else:
                        target_pos = pe_cache_length + attn_cache.target_offset

                    position_ids = mx.array([[target_pos]])

                    logits = model(
                        next_input,
                        cache=caches,
                        position_ids=position_ids,
                        is_reading=False,
                    )
                except Exception as e:
                    import traceback

                    traceback.print_exc()
                    raise

                logits = logits[:, -1, :]

                if repetition_penalty and repetition_penalty != 1.0:
                    if len(all_target_tokens) > 0:
                        context = all_target_tokens[-repetition_context_size:]
                        logits = apply_repetition_penalty(
                            logits, context, repetition_penalty
                        )

                # Mask EOS token only for the next token after reading new source
                if mask_eos_next_token:
                    logits[:, tokenizer.eos_token_id] = float('-inf')
                    mask_eos_next_token = False

                next_token = sampler(logits)
                token_count += 1
                total_tokens_generated += 1

                current_word_tokens.append(int(next_token[0]))

                is_eos = eos_criteria(int(next_token[0]))
                if is_eos:
                    if state.should_read_next_source():
                        # Suppress EOS - there's more source to read
                        current_word_tokens.pop()
                        for cache in caches:
                            if isinstance(cache, DualStreamingCache):
                                cache.pop_target()

                        if len(current_word_tokens) > 0:
                            all_target_tokens.extend(current_word_tokens)
                            word_text = tokenizer.decode(current_word_tokens)
                            state.mark_target_written()

                            yield {
                                "text": word_text,
                                "token_ids": current_word_tokens.copy(),
                                "is_final": False,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "write",
                                "word_complete": True,
                            }

                        last_token = mx.array([[all_target_tokens[-1]]]) if len(all_target_tokens) > 0 else assistant_start_tokens
                        if len(all_target_tokens) <= 0:
                            time.sleep(0.1)
                        next_input = last_token

                        # Read next wait-k words (or all remaining if fewer) to give model more context
                        words_to_read = min(wait_k, len(state.source_seg_len) - state.source_words_read)
                        for _ in range(words_to_read):
                            if not state.should_read_next_source():
                                break

                            tokens_to_read = state.get_source_tokens_to_read()
                            if tokens_to_read == 0:
                                state.mark_source_read()
                                continue

                            source_chunk = source_token_ids[
                                :, source_pos_offset : source_pos_offset + tokens_to_read
                            ]

                            if source_chunk.shape[1] == 0:
                                state.mark_source_read()
                                continue

                            position_ids = mx.arange(
                                source_pos_offset, source_pos_offset + tokens_to_read
                            ).reshape(1, -1)

                            _ = model(
                                source_chunk, cache=caches, position_ids=position_ids, is_reading=True
                            )

                            source_pos_offset += tokens_to_read
                            state.mark_source_read()

                            yield {
                                "text": "",
                                "token_ids": [],
                                "is_final": False,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "read",
                                "word_complete": False,
                            }

                        # Set flag to mask EOS for the next token after reading new source
                        mask_eos_next_token = True
                        state.switch_to_writing()
                        current_word_tokens = []
                        break
                    else:
                        current_word_tokens.pop()

                        if len(current_word_tokens) > 0:
                            all_target_tokens.extend(current_word_tokens)
                            word_text = tokenizer.decode(current_word_tokens)

                            state.mark_target_written()
                            yield {
                                "text": word_text,
                                "token_ids": current_word_tokens.copy(),
                                "is_final": True,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "write",
                                "word_complete": True,
                            }

                        state.finished = True
                        break

                current_word_array = mx.array(current_word_tokens)

                should_stop, remove_last = word_boundary_criteria(
                    current_word_array, token_count
                )

                if max_length_criteria(total_tokens_generated):
                    should_stop = True
                    state.finished = True

                if should_stop:
                    word_finished = True

                    if remove_last and len(current_word_tokens) > 1:
                        removed_token = current_word_tokens.pop()
                        for cache in caches:
                            if isinstance(cache, DualStreamingCache):
                                cache.target_cache.offset -= 1

                    all_target_tokens.extend(current_word_tokens)

                    word_text = tokenizer.decode(current_word_tokens)

                    state.mark_target_written()

                    yield {
                        "text": word_text,
                        "token_ids": current_word_tokens.copy(),
                        "is_final": state.finished
                        or not state.should_write_next_target(),
                        "source_words_read": state.source_words_read,
                        "target_words_generated": state.target_words_generated,
                        "mode": "write",
                        "word_complete": True,
                    }

                    if len(current_word_tokens) > 0:
                        next_input = mx.array([[current_word_tokens[-1]]])

                    if not state.finished and state.should_read_next_source():
                        state.switch_to_reading()
                        current_word_tokens = []
                    elif not state.should_read_next_source():
                        current_word_tokens = []
                    else:
                        state.finished = True

                else:
                    next_input = next_token.reshape(1, 1)

        else:
            state.finished = True


def apply_repetition_penalty(
    logits: mx.array, context_tokens: List[int], penalty: float
) -> mx.array:
    """
    Apply repetition penalty to logits.

    Args:
        logits: Logits array (batch, vocab_size)
        context_tokens: List of recent token IDs
        penalty: Penalty factor (> 1 reduces repetition)

    Returns:
        Modified logits
    """
    if len(context_tokens) == 0 or penalty == 1.0:
        return logits

    for token_id in set(context_tokens):
        if logits[0, token_id] < 0:
            logits[0, token_id] *= penalty
        else:
            logits[0, token_id] /= penalty

    return logits


def generate_streaming(
    model,
    tokenizer,
    prompt: str,
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens: int = 1024,
    system_prompt: str = "",
    split_mode: str = "word",
    verbose: bool = False,
    **kwargs,
) -> str:
    """
    Convenient wrapper for streaming generation.

    Args:
        model: The streaming model
        tokenizer: Tokenizer instance
        prompt: Input prompt text
        wait_k: Wait-k parameter
        max_new_words: Maximum words to generate
        max_tokens: Maximum tokens to generate
        system_prompt: System prompt
        split_mode: How to split text ('word' or 'sentence')
        verbose: Whether to print progress
        **kwargs: Additional generation parameters

    Returns:
        Generated text as a single string

    Example:
        >>> from mlx_lm.models.qwen2_streaming import Model, ModelArgs
        >>> model = Model(args)
        >>> text = generate_streaming(
        ...     model,
        ...     tokenizer,
        ...     "Translate to French: Hello world",
        ...     wait_k=3
        ... )
    """
    from .streaming_data_utils import prepare_streaming_input

    prepared = prepare_streaming_input(
        source_text=prompt,
        tokenizer=tokenizer,
        wait_k=wait_k,
        system_prompt=system_prompt,
        split_mode=split_mode,
        add_space=kwargs.get("add_space", False),
        pe_cache_length=kwargs.get("pe_cache_length", 0),
    )

    if verbose:
        print("=" * 50)
        print(f"Streaming generation with wait-k={wait_k}")
        print("=" * 50)

    generated_text = ""
    for chunk in stream_generate_streaming(
        model=model,
        tokenizer=tokenizer,
        source_token_ids=prepared["source_token_ids"],
        source_seg_len=prepared["source_seg_len"],
        wait_k=wait_k,
        max_new_words=max_new_words,
        max_tokens=max_tokens,
        assistant_start_tokens=prepared["assistant_start_tokens"],
        split_mode=split_mode,
        end_token=prepared["metadata"]["end_token"],
        **kwargs,
    ):
        if chunk["mode"] == "write" and chunk["text"]:
            generated_text += chunk["text"]
            if verbose:
                print(chunk["text"], end="", flush=True)

    if verbose:
        print()
        print("=" * 50)
        print(f"Generated {len(generated_text.split())} words")
        print("=" * 50)

    return generated_text

import time
from typing import Generator, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

from .streaming_data_utils import StreamingDataPreparator
from .streaming_generate import GenerationResponse, stream_generate_streaming
from .tokenizer_utils import TokenizerWrapper


def stream_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    prompt: Union[str, mx.array, List[int]],
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens_per_word: int = 50,
    system_prompt: str = "Translate the following English paragraph to French",
    temp: float = 0.0,
    top_p: float = 1.0,
    min_p: float = 0.0,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: int = 20,
    **kwargs,
) -> Generator[GenerationResponse, None, None]:
    """
    A generator producing text using StreamingLLM's wait-k policy.

    This function enables streaming generation where the model processes
    source text incrementally and generates output with a configurable lag
    (wait-k policy). This is useful for simultaneous translation, streaming
    transcription, and real-time text processing.

    Args:
        model (nn.Module): The model to use for generation. Must support streaming
            mode (e.g., Qwen2ModelStreaming).
        tokenizer (PreTrainedTokenizer): The tokenizer.
        prompt (Union[str, mx.array, List[int]]): The input prompt string or
            integer tokens. This is treated as the "source" text in streaming mode.
            NOTE: This should be ONLY the source text to process, not including the
            instruction/task description.
        wait_k (int): Number of source words to wait for before generating each
            target word. Default: ``3``.
        max_new_words (Optional[int]): Maximum number of words to generate. If None,
            generates until end of source. Default: ``None``.
        max_tokens_per_word (int): Maximum tokens per generated word. Default: ``50``.
        system_prompt (str): The system/instruction prompt that describes the task
            (e.g., "Translate the following English paragraph to French"). This is
            kept separate from the source text and NOT segmented word-by-word.
            Default: ``"Translate the following English paragraph to French"``.
        temp (float): Sampling temperature. Default: ``0.0`` (greedy).
        top_p (float): Sampling top-p. Default: ``1.0``.
        min_p (float): Sampling min-p. Default: ``0.0``.
        repetition_penalty (Optional[float]): Repetition penalty factor. Default: ``None``.
        repetition_context_size (int): Context size for repetition penalty. Default: ``20``.
        kwargs: Additional keyword arguments (ignored for compatibility).

    Yields:
        GenerationResponse: An instance containing the generated text segment and
            associated metadata. The response includes:
            - text: Generated text for the current word
            - token: Last token of the word
            - word_complete: True when a word boundary is reached
            - source_words_read: Number of source words processed so far
            - target_words_generated: Number of target words generated so far

    Example:
        >>> from mlx_streaming_llm import load, stream_generate
        >>>
        >>> model, tokenizer = load("Qwen/Qwen2.5-0.5B-Instruct")
        >>> source_text = "Hello, how are you?"
        >>> system_prompt = "Translate the following English paragraph to French"
        >>>
        >>> for response in stream_generate(
        ...     model, tokenizer, source_text,
        ...     wait_k=3,
        ...     system_prompt=system_prompt
        ... ):
        ...     if response.word_complete:
        ...         print(response.text, end=' ', flush=True)
        ...         print(f"[{response.source_words_read} source words]")
    """
    if not isinstance(tokenizer, TokenizerWrapper):
        tokenizer = TokenizerWrapper(tokenizer)

    preparator = StreamingDataPreparator(tokenizer, system_prompt=system_prompt)

    if isinstance(prompt, str):
        formatted_text, token_ids, seg_lens = preparator.prepare_source_text(prompt)

    elif isinstance(prompt, (list, mx.array)):
        token_ids = prompt.tolist() if isinstance(prompt, mx.array) else prompt
        seg_lens = [1] * len(token_ids)
    else:
        raise ValueError("Prompt must be a string, list of ints, or mx.array")

    tic = time.perf_counter()
    prompt_time = 0
    first_token = True
    total_tokens = 0
    source_words_total = len(seg_lens)

    if not isinstance(token_ids, mx.array):
        token_ids = mx.array([token_ids])
    elif token_ids.ndim == 1:
        token_ids = token_ids.reshape(1, -1)

    assistant_start_tokens = mx.array([preparator.assistant_tokens])

    pe_cache_length = int(token_ids.shape[1])

    for output in stream_generate_streaming(
        model=model,
        tokenizer=tokenizer._tokenizer if hasattr(tokenizer, "_tokenizer") else tokenizer,
        source_token_ids=token_ids,
        source_seg_len=seg_lens,
        wait_k=wait_k,
        max_new_words=max_new_words,
        max_tokens_per_word=max_tokens_per_word,
        assistant_start_tokens=assistant_start_tokens,
        pe_cache_length=pe_cache_length,
        temp=temp,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        repetition_context_size=repetition_context_size,
    ):
        if first_token:
            prompt_time = time.perf_counter() - tic
            first_token = False
            tic = time.perf_counter()

        total_tokens += 1
        elapsed = time.perf_counter() - tic

        response = GenerationResponse(
            text=output.get("text", ""),
            token=output.get("token", 0),
            logprobs=mx.array([0.0]),
            from_draft=False,
            prompt_tokens=len(token_ids),
            prompt_tps=len(token_ids) / prompt_time if prompt_time > 0 else 0.0,
            generation_tokens=total_tokens,
            generation_tps=total_tokens / elapsed if elapsed > 0 else 0.0,
            peak_memory=mx.get_peak_memory() / 1e9,
            finish_reason=None,
        )

        response.word_complete = output.get("word_complete", False)
        response.source_words_read = output.get("source_words_read", 0)
        response.target_words_generated = output.get("target_words_generated", 0)

        yield response
