import time
from dataclasses import dataclass
from typing import Dict, Generator, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

from .models.streaming_cache import DualStreamingCache
from .sample_utils import make_sampler
from .streaming_stopping_criteria import (
    EOSStoppingCriteria,
    MaxLengthStoppingCriteria,
    WordBoundaryStoppingCriteria,
)
from .streaming_utils import StreamingState, calculate_wait_words
from .tokenizer_utils import TokenizerWrapper


@dataclass
class GenerationResponse:
    """
    The output of streaming generation.

    Args:
        text (str): The next segment of decoded text. This can be an empty string.
        token (int): The next token.
        logprobs (mx.array): A vector of log probabilities.
        from_draft (bool): Whether the token was generated by the draft model.
        prompt_tokens (int): The number of tokens in the prompt.
        prompt_tps (float): The prompt processing tokens-per-second.
        generation_tokens (int): The number of generated tokens.
        generation_tps (float): The tokens-per-second for generation.
        peak_memory (float): The peak memory used so far in GB.
        finish_reason (str): The reason the response is being sent: "length", "stop" or `None`
        word_complete (bool): True when a word boundary is reached (streaming-specific).
        source_words_read (int): Number of source words processed so far (streaming-specific).
        target_words_generated (int): Number of target words generated (streaming-specific).
    """

    text: str
    token: int
    logprobs: mx.array
    from_draft: bool
    prompt_tokens: int
    prompt_tps: float
    generation_tokens: int
    generation_tps: float
    peak_memory: float
    finish_reason: Optional[str] = None
    word_complete: bool = False
    source_words_read: int = 0
    target_words_generated: int = 0


def stream_generate_streaming(
    model,
    tokenizer,
    source_reader,
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens: int = 1024,
    assistant_start_tokens: Optional[mx.array] = None,
    pe_cache_length: int = 0,
    split_mode: str = "sentence",
    end_token: str = "<|im_end|>",
    temp: float = 0.0,
    top_p: float = 1.0,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: Optional[int] = 20,
    **kwargs,
) -> Generator[Dict, None, None]:
    """
    Generate tokens using streaming policy (wait-k) with queue-based input.

    This implements the core streaming generation algorithm from StreamingLLM,
    alternating between reading source tokens and writing target tokens based
    on the wait-k policy. Source text is read from a queue, blocking when
    more input is needed.

    Reference: StreamingLLM's _sample_streaming in generation/generate.py:947-1206

    Args:
        model: The streaming model (with DualStreamingCache support)
        tokenizer: Tokenizer instance
        source_reader: QueueSourceReader that provides tokens from a queue
        wait_k: Wait-k parameter (wait for k source words before generating)
        max_new_words: Maximum number of target words to generate
        max_tokens: Maximum total tokens to generate
        assistant_start_tokens: Token IDs for assistant message start
        pe_cache_length: Starting position ID for target tokens (default 0)
        split_mode: Segmentation mode ('word' or 'sentence')
        end_token: End instruction token
        temp: Sampling temperature
        top_p: Top-p sampling parameter
        repetition_penalty: Repetition penalty factor
        repetition_context_size: Context size for repetition penalty

    Yields:
        Dictionary with:
        - text: Generated text for current word
        - token_ids: Generated token IDs
        - is_final: Whether this is the final word
        - source_words_read: Number of source words read so far
        - target_words_generated: Number of target words generated
        - mode: Current mode ('read' or 'write')
        - word_complete: True when a word has been completed (write mode), False otherwise

    Algorithm:
        1. Initialize dual caches and state
        2. While not finished:
           IF reading mode:
             - Read next source word from queue (BLOCKS if queue empty)
             - Update source cache
             - Switch to writing if wait-k satisfied
           ELSE (writing mode):
             - Generate tokens until word boundary
             - Update target cache
             - Switch to reading if more source available

    Example:
        >>> from queue import Queue
        >>> from mlx_lm.streaming_data_utils import QueueSourceReader
        >>>
        >>> source_queue = Queue()
        >>> reader = QueueSourceReader(source_queue, tokenizer)
        >>>
        >>> # In another thread:
        >>> source_queue.put("Hello world")
        >>> source_queue.put(None)
        >>>
        >>> for chunk in stream_generate_streaming(
        ...     model,
        ...     tokenizer,
        ...     reader,
        ...     wait_k=2
        ... ):
        ...     print(chunk["text"], end="", flush=True)
    """

    sampler = make_sampler(
        temp=temp,
        top_p=top_p,
        min_p=kwargs.get("min_p", 0.0),
        top_k=kwargs.get("top_k", 0),
    )

    if hasattr(model, "make_cache"):
        caches = model.make_cache()
    else:
        num_layers = len(model.layers)
        caches = [DualStreamingCache() for _ in range(num_layers)]

    attn_cache = None
    for cache in caches:
        if isinstance(cache, DualStreamingCache):
            attn_cache = cache
            break

    if attn_cache is None:
        attn_cache = caches[0]

    state = StreamingState(
        wait_k=wait_k,
        max_target_words=max_new_words,
        pe_cache_length=pe_cache_length,
    )

    word_boundary_criteria = WordBoundaryStoppingCriteria(
        tokenizer=tokenizer, max_tokens_per_word=5, end_tokens=[end_token]
    )

    max_length_criteria = MaxLengthStoppingCriteria(max_length=max_tokens)

    eos_criteria = EOSStoppingCriteria(eos_token_id=tokenizer.eos_token_id)

    all_target_tokens = []
    current_word_tokens = []
    total_tokens_generated = 0
    mask_eos_next_token = False

    if assistant_start_tokens is not None:
        next_input = assistant_start_tokens
    else:
        if hasattr(tokenizer, "bos_token_id") and tokenizer.bos_token_id is not None:
            next_input = mx.array([[tokenizer.bos_token_id]])
        elif hasattr(tokenizer, "eos_token_id") and tokenizer.eos_token_id is not None:
            next_input = mx.array([[tokenizer.eos_token_id]])
        else:
            next_input = mx.array([[0]])

    source_pos_offset = 0

    while not state.finished:

        if state.is_reading and state.should_read_next_source():
            # Try to get next word tokens from queue (NON-BLOCKING)
            # This allows model to continue generating if no more source is available yet
            result = source_reader.get_next_word_tokens(blocking=False)

            if result is None:
                # Check if stream actually ended or just no data available yet
                if source_reader.is_stream_ended():
                    # Stream ended
                    state.mark_source_stream_ended()
                    if state.check_wait_k_policy():
                        state.switch_to_writing()
                        current_word_tokens = []
                    continue
                else:
                    # No data available yet, but stream hasn't ended
                    # Switch to writing mode to continue generating
                    state.switch_to_writing()
                    current_word_tokens = []
                    continue

            source_chunk, tokens_to_read = result

            if tokens_to_read == 0:
                continue

            # Add this segment to state
            state.add_source_segment(tokens_to_read)

            position_ids = mx.arange(
                source_pos_offset, source_pos_offset + tokens_to_read
            ).reshape(1, -1)

            _ = model(
                source_chunk, cache=caches, position_ids=position_ids, is_reading=True
            )

            source_pos_offset += tokens_to_read
            state.mark_source_read()

            yield {
                "text": "",
                "token_ids": [],
                "is_final": False,
                "source_words_read": state.source_words_read,
                "target_words_generated": state.target_words_generated,
                "mode": "read",
                "word_complete": False,
            }

            if state.check_wait_k_policy():
                state.switch_to_writing()
                current_word_tokens = []

        elif not state.is_reading and state.should_write_next_target():
            token_count = 0
            word_finished = False

            while not word_finished:

                try:
                    if attn_cache.target_offset == 0:
                        target_pos = pe_cache_length + len(current_word_tokens)
                    else:
                        target_pos = pe_cache_length + attn_cache.target_offset

                    position_ids = mx.array([[target_pos]])

                    # Pass recency info as a dict - bias will be computed in attention layer
                    # with the correct cache size
                    recency_bias = {
                        "source_timestamps": state.source_token_timestamps,
                        "source_offset": attn_cache.source_offset,
                        "current_time": time.time(),
                        "recency_window": 2.0,
                        "source_boost": 5.0,
                        "target_dampening": 1.0,
                    }

                    logits = model(
                        next_input,
                        cache=caches,
                        position_ids=position_ids,
                        is_reading=False,
                        recency_bias=recency_bias,
                    )
                except Exception as e:
                    import traceback

                    traceback.print_exc()
                    raise

                logits = logits[:, -1, :]

                if repetition_penalty and repetition_penalty != 1.0:
                    if len(all_target_tokens) > 0:
                        context = all_target_tokens[-repetition_context_size:]
                        logits = apply_repetition_penalty(
                            logits, context, repetition_penalty
                        )

                # Mask EOS token only for the next token after reading new source
                if mask_eos_next_token:
                    logits[:, tokenizer.eos_token_id] = float("-inf")
                    mask_eos_next_token = False

                next_token = sampler(logits)
                token_count += 1
                total_tokens_generated += 1

                current_word_tokens.append(int(next_token[0]))

                is_eos = eos_criteria(int(next_token[0]))
                if is_eos:
                    if state.should_read_next_source():
                        # Suppress EOS - there's more source to read
                        current_word_tokens.pop()
                        for cache in caches:
                            if isinstance(cache, DualStreamingCache):
                                cache.pop_target()

                        if len(current_word_tokens) > 0:
                            all_target_tokens.extend(current_word_tokens)
                            word_text = tokenizer.decode(current_word_tokens)
                            state.mark_target_written()

                            yield {
                                "text": word_text,
                                "token_ids": current_word_tokens.copy(),
                                "is_final": False,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "write",
                                "word_complete": True,
                            }

                        last_token = (
                            mx.array([[all_target_tokens[-1]]])
                            if len(all_target_tokens) > 0
                            else assistant_start_tokens
                        )
                        next_input = last_token

                        # Read next wait-k words - MUST BLOCK and wait for more source after suppressing EOS
                        words_read = 0
                        for _ in range(wait_k):
                            if not state.should_read_next_source():
                                break

                            # BLOCKING read - we must wait for more source after suppressing EOS
                            result = source_reader.get_next_word_tokens(blocking=True)
                            if result is None:
                                state.mark_source_stream_ended()
                                break

                            source_chunk, tokens_to_read = result
                            if tokens_to_read == 0:
                                continue

                            state.add_source_segment(tokens_to_read)

                            position_ids = mx.arange(
                                source_pos_offset, source_pos_offset + tokens_to_read
                            ).reshape(1, -1)

                            _ = model(
                                source_chunk,
                                cache=caches,
                                position_ids=position_ids,
                                is_reading=True,
                            )

                            source_pos_offset += tokens_to_read
                            state.mark_source_read()
                            words_read += 1

                            yield {
                                "text": "",
                                "token_ids": [],
                                "is_final": False,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "read",
                                "word_complete": False,
                            }

                        # Check if we successfully read wait_k words
                        if words_read == wait_k:
                            # We read new source, mask EOS and continue generation
                            mask_eos_next_token = True
                            state.switch_to_writing()
                            current_word_tokens = []
                            break
                        else:
                            # Stream ended without reading new words - finish generation
                            state.finished = True
                            break
                    else:
                        current_word_tokens.pop()

                        if len(current_word_tokens) > 0:
                            all_target_tokens.extend(current_word_tokens)
                            word_text = tokenizer.decode(current_word_tokens)

                            state.mark_target_written()
                            yield {
                                "text": word_text,
                                "token_ids": current_word_tokens.copy(),
                                "is_final": True,
                                "source_words_read": state.source_words_read,
                                "target_words_generated": state.target_words_generated,
                                "mode": "write",
                                "word_complete": True,
                            }

                        state.finished = True
                        break

                current_word_array = mx.array(current_word_tokens)

                should_stop, remove_last = word_boundary_criteria(
                    current_word_array, token_count
                )

                if max_length_criteria(total_tokens_generated):
                    should_stop = True
                    state.finished = True

                if should_stop:
                    word_finished = True

                    if remove_last and len(current_word_tokens) > 1:
                        removed_token = current_word_tokens.pop()
                        for cache in caches:
                            if isinstance(cache, DualStreamingCache):
                                cache.target_cache.offset -= 1

                    all_target_tokens.extend(current_word_tokens)

                    word_text = tokenizer.decode(current_word_tokens)

                    state.mark_target_written()

                    yield {
                        "text": word_text,
                        "token_ids": current_word_tokens.copy(),
                        "is_final": state.finished
                        or not state.should_write_next_target(),
                        "source_words_read": state.source_words_read,
                        "target_words_generated": state.target_words_generated,
                        "mode": "write",
                        "word_complete": True,
                    }

                    if len(current_word_tokens) > 0:
                        next_input = mx.array([[current_word_tokens[-1]]])

                    if not state.finished and state.should_read_next_source():
                        state.switch_to_reading()
                        current_word_tokens = []
                    elif not state.should_read_next_source():
                        current_word_tokens = []
                    else:
                        state.finished = True

                else:
                    next_input = next_token.reshape(1, 1)

        else:
            state.finished = True


def apply_repetition_penalty(
    logits: mx.array, context_tokens: List[int], penalty: float
) -> mx.array:
    """
    Apply repetition penalty to logits.

    Args:
        logits: Logits array (batch, vocab_size)
        context_tokens: List of recent token IDs
        penalty: Penalty factor (> 1 reduces repetition)

    Returns:
        Modified logits
    """
    if len(context_tokens) == 0 or penalty == 1.0:
        return logits

    for token_id in set(context_tokens):
        if logits[0, token_id] < 0:
            logits[0, token_id] *= penalty
        else:
            logits[0, token_id] /= penalty

    return logits


# Removed: generate_streaming() - static text API no longer supported
# Use stream_generate() with a Queue instead

import time
from typing import Generator, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

from .streaming_data_utils import StreamingDataPreparator
from .streaming_generate import GenerationResponse, stream_generate_streaming
from .tokenizer_utils import TokenizerWrapper


def stream_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    source_queue,
    wait_k: int = 3,
    max_new_words: Optional[int] = None,
    max_tokens_per_word: int = 50,
    system_prompt: str = "Translate the following English paragraph to French",
    split_mode: str = "word",
    pe_cache_length: Optional[int] = None,
    temp: float = 0.0,
    top_p: float = 1.0,
    min_p: float = 0.0,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: int = 20,
    **kwargs,
) -> Generator[GenerationResponse, None, None]:
    """
    A generator producing text using StreamingLLM's wait-k policy with queue-based input.

    This function enables real-time streaming generation where the model processes
    source text incrementally as it arrives from a queue, blocking when waiting for
    more input. This is ideal for simultaneous translation, live transcription,
    and real-time text processing scenarios.

    Args:
        model (nn.Module): The model to use for generation. Must support streaming
            mode (e.g., Qwen2ModelStreaming).
        tokenizer (PreTrainedTokenizer): The tokenizer.
        source_queue (Queue): Queue object that receives source text chunks (strings)
            and None as end-of-stream signal. The generator blocks when the queue
            is empty, waiting for more input.
        wait_k (int): Number of source words to wait for before generating each
            target word. Default: ``3``.
        max_new_words (Optional[int]): Maximum number of words to generate. If None,
            generates until source stream ends. Default: ``None``.
        max_tokens_per_word (int): Maximum tokens per generated word. Default: ``50``.
        system_prompt (str): The system/instruction prompt that describes the task
            (e.g., "Translate the following English paragraph to French"). This is
            kept separate from the source text and NOT segmented word-by-word.
            Default: ``"Translate the following English paragraph to French"``.
        split_mode (str): How to split source text ('word' or 'sentence'). Default: ``'word'``.
        pe_cache_length (Optional[int]): Position encoding offset for target tokens. This creates
            a separation between source and target position IDs to prevent overlap. Source tokens
            use positions [0, 1, 2, ...], while target tokens use [pe_cache_length, pe_cache_length+1, ...].
            Should be set larger than your maximum expected source token length. If None, defaults to
            8192 for models with max_position_embeddings >= 32768, otherwise uses max_position_embeddings // 2.
            For longer inputs, increase this value (e.g., 16384 or 32768). Default: ``None`` (auto-detect).
        temp (float): Sampling temperature. Default: ``0.0`` (greedy).
        top_p (float): Sampling top-p. Default: ``1.0``.
        min_p (float): Sampling min-p. Default: ``0.0``.
        repetition_penalty (Optional[float]): Repetition penalty factor. Default: ``None``.
        repetition_context_size (int): Context size for repetition penalty. Default: ``20``.
        kwargs: Additional keyword arguments (ignored for compatibility).

    Yields:
        GenerationResponse: An instance containing the generated text segment and
            associated metadata. The response includes:
            - text: Generated text for the current word
            - token: Last token of the word
            - word_complete: True when a word boundary is reached
            - source_words_read: Number of source words processed so far
            - target_words_generated: Number of target words generated so far

    Example:
        >>> from queue import Queue
        >>> from threading import Thread
        >>> from mlx_lm import load, stream_generate
        >>>
        >>> model, tokenizer = load("Qwen/Qwen2.5-0.5B-Instruct")
        >>> source_queue = Queue()
        >>>
        >>> # Feed text from another thread (e.g., from STT)
        >>> def feed_text():
        ...     source_queue.put("Hello ")
        ...     source_queue.put("world ")
        ...     source_queue.put(None)  # Signal end of stream
        >>>
        >>> Thread(target=feed_text).start()
        >>>
        >>> for response in stream_generate(
        ...     model, tokenizer, source_queue,
        ...     wait_k=3,
        ...     system_prompt="Translate to French"
        ... ):
        ...     if response.word_complete:
        ...         print(response.text, end=' ', flush=True)
    """
    if not isinstance(tokenizer, TokenizerWrapper):
        tokenizer = TokenizerWrapper(tokenizer)

    # Create queue source reader
    from .streaming_data_utils import QueueSourceReader

    source_reader = QueueSourceReader(
        queue=source_queue,
        tokenizer=(
            tokenizer._tokenizer if hasattr(tokenizer, "_tokenizer") else tokenizer
        ),
        split_mode=split_mode,
        system_prompt=system_prompt,
    )

    # Auto-detect pe_cache_length if not provided
    # This creates position ID separation: source uses [0, 1, 2, ...], target uses [pe_cache_length, ...]
    if pe_cache_length is None:
        # Try to get max_position_embeddings from model config
        if hasattr(model, "config") and hasattr(
            model.config, "max_position_embeddings"
        ):
            max_pos = model.config.max_position_embeddings
            # For models with large context (32k+), use 8192 as a reasonable default
            # For smaller models, use half the max to leave room for both source and target
            if max_pos >= 32768:
                pe_cache_length = 8192
            else:
                pe_cache_length = max_pos // 2
        else:
            # Fallback default for models without config
            pe_cache_length = 8192

    # Assistant start tokens
    assistant_template = "<|im_start|>assistant\n"
    assistant_tokens = tokenizer.encode(assistant_template, add_special_tokens=False)
    assistant_start_tokens = mx.array([assistant_tokens])

    tic = time.perf_counter()
    prompt_time = 0
    first_token = True
    total_tokens = 0

    for output in stream_generate_streaming(
        model=model,
        tokenizer=(
            tokenizer._tokenizer if hasattr(tokenizer, "_tokenizer") else tokenizer
        ),
        source_reader=source_reader,
        wait_k=wait_k,
        max_new_words=max_new_words,
        max_tokens_per_word=max_tokens_per_word,
        assistant_start_tokens=assistant_start_tokens,
        pe_cache_length=pe_cache_length,
        temp=temp,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        repetition_context_size=repetition_context_size,
    ):
        if first_token:
            prompt_time = time.perf_counter() - tic
            first_token = False
            tic = time.perf_counter()

        total_tokens += 1
        elapsed = time.perf_counter() - tic

        # For queue-based streaming, we don't know prompt tokens upfront
        # Use source_words_read as a proxy
        prompt_tokens = output.get("source_words_read", 0)

        response = GenerationResponse(
            text=output.get("text", ""),
            token=output.get("token", 0),
            logprobs=mx.array([0.0]),
            from_draft=False,
            prompt_tokens=prompt_tokens,
            prompt_tps=prompt_tokens / prompt_time if prompt_time > 0 else 0.0,
            generation_tokens=total_tokens,
            generation_tps=total_tokens / elapsed if elapsed > 0 else 0.0,
            peak_memory=mx.get_peak_memory() / 1e9,
            finish_reason=None,
        )

        response.word_complete = output.get("word_complete", False)
        response.source_words_read = output.get("source_words_read", 0)
        response.target_words_generated = output.get("target_words_generated", 0)

        yield response
