#!/usr/bin/env python3
"""
StreamingLLM Live Visualization Example - Juno Conversational Assistant

This example demonstrates a streaming conversational assistant (Juno) that:
1. Continuously processes a stream of diarized speaker segments
2. Uses LLM semantic understanding to determine when to speak
3. Can be interrupted and adapts to the conversation flow
4. Doesn't require wake words - always listening and processing
"""

import sys

from mlx_lm import load_streaming, stream_generate_streaming_llm

# Simulated stream of diarized speaker segments
LONG_PARAGRAPH = """
[Bob]: Hey Alice, how was your weekend? Did you manage to go to that new restaurant?
[Alice]: Oh, it was great! Yes, we finally went to that Japanese place downtown. The sushi was incredible.
[Bob]: That sounds amazing. I've been meaning to try it. What did you order?
[Alice]: We had the omakase menu. The chef's selection was really impressive.
[Bob]: Juno, can you tell us about omakase dining?
[Alice]: Yes, I'm curious too. What makes it special?
[Bob]: And maybe recommend some dishes we should try if we're new to Japanese cuisine?
[Alice]: That would be helpful. My friend is visiting next week and I'd love to take them somewhere nice.
[Bob]: Juno, also could you suggest some good Japanese restaurants in the downtown area?
[Alice]: Oh, and while you're at it, can you explain the difference between sushi and sashimi?
[Bob]: Right, I always get confused about that.
[Alice]: Me too! And what about wasabi etiquette? I never know how much to use.
[Bob]: Juno, are you there? We'd love your input on Japanese dining.
[Alice]: Maybe she's processing all our questions. We did ask a lot at once!
[Bob]: True. Juno, just start with whatever you think is most important about omakase.
[Alice]: Yes, take your time. We're listening.
"""

# System prompt for Juno
JUNO_SYSTEM_PROMPT = """You are Juno, an AI assistant. You are NOT participating in the conversation—you are OBSERVING a live conversation between other people (like Bob and Alice).

## Critical Rules:
1. You are LISTENING to a conversation, not part of it
2. You will see messages in the format [Speaker]: text
3. You are ONLY "Juno" - you are NOT Bob, NOT Alice, NOT any other speaker
4. DO NOT respond or speak unless someone is clearly addressing you (Juno) and not someone else
5. When processing conversation, note who is speaking, but do NOT speak for them

## When Someone Addresses You:
- Look for phrases that address you by name or are clearly intended for you
- Only THEN should you call the `speak` tool and respond
- Respond as Juno, the AI assistant who has been listening
- Do not echo the conversation while observing - your thoughts should be your own
- Be mindful of interruptions - someone may ask you something, then change the conversation. When this happens, you should stop talking.

## Tools Available:
<tools>
speak: Begin speaking (only when addressed!)
stop_speaking: Stop speaking
</tools>

To call: <tool_call>{"name": "function_name"}</tool_call>

## Response Protocol:
1. When addressed, call: <tool_call>{"name": "speak", "arguments": {}}</tool_call>
2. Generate your response (ONE complete thought)
3. IMMEDIATELY call: <tool_call>{"name": "stop_speaking", "arguments": {}}</tool_call>
4. DO NOT continue the conversation for others
5. DO NOT generate [Speaker]: tags for other participants

## Prohibited Behaviors:
- ❌ Generating speech for Bob, Alice, or other participants
- ❌ Writing [Bob]: or [Alice]: in your output
- ❌ Continuing past your answer to imagine their responses
- ❌ Forgetting to call stop_speaking

Remember: You are an OBSERVER until addressed."""


def live_streaming_visualization(model, tokenizer, source_text, wait_k=50):
    """
    Live visualization showing input stream and output stream separately.

    This matches StreamingLLM's streaming_eval.py behavior where:
    - Input words appear as they're READ by the model
    - Output words appear as they're GENERATED by the model

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to translate/process
        wait_k: Wait-k policy parameter
    """
    import os
    import shutil

    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Get terminal width for proper wrapping calculation
    try:
        terminal_width = shutil.get_terminal_size().columns
    except:
        terminal_width = 80  # fallback

    # Parse source words for display
    source_words = source_text.strip().split()

    # Track what we've displayed
    last_source_read = 0
    output_words = []

    # Build complete strings for display
    current_input_text = ""
    current_output_text = ""

    # Helper function to calculate number of lines text will occupy
    def calculate_lines(text, width=terminal_width):
        if not text:
            return 1
        # Account for "streaming-input: " or "streaming-output: " prefix on first line
        lines = 0
        current_line_length = 0
        for char in text:
            if char == "\n":
                lines += 1
                current_line_length = 0
            else:
                current_line_length += 1
                if current_line_length >= width:
                    lines += 1
                    current_line_length = 1
        return max(1, lines + (1 if current_line_length > 0 else 0))

    # Clear screen and set up initial display
    if os.name == "nt":  # Windows
        os.system("cls")
    else:  # Unix/Linux/Mac
        sys.stdout.write("\033[2J\033[H")  # Clear screen and move to top

    # Print initial headers
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Remember the starting position for our display area
    sys.stdout.write("\033[s")  # Save cursor position

    # Reserve space for the display
    print("conversation-stream (incoming):")
    print()  # Reserve multiple lines for input
    print()  # Separator
    print("juno-response:")
    print()  # Reserve multiple lines for output
    print()  # Extra space

    # Move back to saved position
    sys.stdout.write("\033[u")  # Restore cursor position

    # Stream generation - Juno processing conversation
    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=source_text,  # The diarized speaker stream
        wait_k=wait_k,
        max_new_words=2000,
        system_prompt=JUNO_SYSTEM_PROMPT,
        temp=0.5,  # More natural conversation
        top_p=0.8,
    ):
        # Update displays by completely redrawing the display area
        needs_redraw = False

        # Check if input stream needs update
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            words_to_show = source_words[: response.source_words_read]
            current_input_text = " ".join(words_to_show)
            last_source_read = response.source_words_read
            needs_redraw = True

        # Check if output stream needs update
        if hasattr(response, "word_complete") and response.word_complete:
            output_words.append(response.text)
            current_output_text = " ".join(output_words)
            needs_redraw = True

        if needs_redraw:
            # Clear the display area and redraw everything
            sys.stdout.write("\033[u")  # Restore to saved position
            sys.stdout.write("\033[J")  # Clear from cursor to end of screen

            # Redraw the display
            print("conversation-stream (incoming):")
            print(current_input_text if current_input_text else "")
            print()  # Separator
            print("juno-response (when speaking):")
            print(current_output_text if current_output_text else "")

            sys.stdout.flush()

    # Final newline after generation completes
    print()
    print()
    print("=" * 80)
    print(f"Source words read: {last_source_read}")
    print(f"Target words generated: {len(output_words)}")

    if hasattr(response, "generation_tps"):
        print(f"Generation speed: {response.generation_tps:.2f} tokens/sec")
        print(f"Peak memory: {response.peak_memory:.2f} GB")
    print("=" * 80)


def simple_streaming_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Simpler visualization without ANSI cursor control.
    Shows input/output with periodic updates.

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()

    print("conversation-stream (incoming):")
    displayed_input_words = []

    print("\njuno-response (when speaking):")
    output_words = []

    last_source_read = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=source_text,  # The diarized speaker stream
        wait_k=wait_k,
        max_new_words=500,
        system_prompt=JUNO_SYSTEM_PROMPT,
        temp=0.7,
    ):
        # Show source words as they're read
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            new_words = source_words[last_source_read : response.source_words_read]
            for word in new_words:
                displayed_input_words.append(word)
            last_source_read = response.source_words_read

        # Show target words as they're generated
        if hasattr(response, "word_complete") and response.word_complete:
            print(response.text, end=" ", flush=True)
            output_words.append(response.text)

    print()
    print()
    print("=" * 80)
    print(f"Input processed: {' '.join(source_words[:last_source_read])}")
    print(f"Source words: {last_source_read}/{len(source_words)}")
    print(f"Target words: {len(output_words)}")
    print("=" * 80)


def side_by_side_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Side-by-side visualization showing conversation stream and Juno's response.

    Shows:
    - Which conversation segments have been processed
    - Which response words Juno has generated
    - Current lag between input and output

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Conversation stream text
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"CONVERSATION STREAM vs JUNO RESPONSE (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()

    print(f"{'Conversation Stream (Read)':<40} | {'Juno Response (Generated)':<40}")
    print("-" * 80)

    source_display = []
    target_display = []

    last_source_read = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=source_text,  # The diarized speaker stream
        wait_k=wait_k,
        max_new_words=500,
        system_prompt=JUNO_SYSTEM_PROMPT,
        temp=0.7,
    ):
        # Update source display
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            new_words = source_words[last_source_read : response.source_words_read]
            source_display.extend(new_words)
            last_source_read = response.source_words_read

        # Update target display
        if hasattr(response, "word_complete") and response.word_complete:
            target_display.append(response.text)

            # Print current state (truncate if too long)
            source_str = " ".join(source_display)
            target_str = " ".join(target_display)

            # Truncate for display
            if len(source_str) > 38:
                source_str = source_str[:35] + "..."
            if len(target_str) > 38:
                target_str = target_str[:35] + "..."

            # Clear line and print
            sys.stdout.write("\r")
            sys.stdout.write(f"{source_str:<40} | {target_str:<40}")
            sys.stdout.flush()

    print()
    print()
    print("=" * 80)
    print(f"Conversation processed: {' '.join(source_words[:last_source_read])}")
    print()
    print(f"Juno's response: {' '.join(target_display)}")
    print()
    print(f"Processing lag: {last_source_read - len(target_display)} words")
    print("=" * 80)


def word_by_word_trace(model, tokenizer, source_text, wait_k=3):
    """
    Detailed word-by-word trace showing streaming progression.

    For each target word generated, shows:
    - Number of source words read at that point
    - Current lag
    - Cumulative output

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"WORD-BY-WORD STREAMING TRACE (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()
    print(f"Source: {source_text}")
    print(f"Source words: {len(source_words)}")
    print()
    print(
        f"{'Step':<6} {'Target Word':<20} {'Src Read':<10} {'Tgt Gen':<10} {'Lag':<6}"
    )
    print("-" * 80)

    step = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=source_text,  # The diarized speaker stream
        wait_k=wait_k,
        max_new_words=500,
        system_prompt=JUNO_SYSTEM_PROMPT,
        temp=0.7,
    ):
        if hasattr(response, "word_complete") and response.word_complete:
            step += 1
            word = response.text
            src_read = (
                response.source_words_read
                if hasattr(response, "source_words_read")
                else "?"
            )
            tgt_gen = (
                response.target_words_generated
                if hasattr(response, "target_words_generated")
                else "?"
            )

            if isinstance(src_read, int) and isinstance(tgt_gen, int):
                lag = src_read - tgt_gen
            else:
                lag = "?"

            print(f"{step:<6} {word:<20} {src_read:<10} {tgt_gen:<10} {lag:<6}")

    print()
    print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="StreamingLLM Live Visualization")
    parser.add_argument(
        "--model",
        type=str,
        default="mlx-community/Qwen3-VL-32B-Instruct-4bit",
        help="Model path",
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="live",
        choices=["live", "simple", "side-by-side", "trace", "all"],
        help="Visualization mode",
    )
    parser.add_argument(
        "--wait-k",
        type=int,
        default=7,
        help="Wait-k value",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Custom input prompt (default: long paragraph)",
    )

    args = parser.parse_args()

    # Use long paragraph by default
    if args.prompt is None:
        source_text = LONG_PARAGRAPH.strip()
    else:
        source_text = args.prompt

    print(f"Loading model: {args.model}")
    model, tokenizer = load_streaming(args.model)
    print("Model loaded successfully!\n")

    if args.mode == "live":
        live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "simple":
        simple_streaming_visualization(
            model, tokenizer, source_text, wait_k=args.wait_k
        )
    elif args.mode == "side-by-side":
        side_by_side_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "trace":
        word_by_word_trace(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "all":
        print("\n" + "=" * 80)
        print("MODE 1: SIMPLE STREAMING")
        print("=" * 80 + "\n")
        simple_streaming_visualization(
            model, tokenizer, source_text, wait_k=args.wait_k
        )

        print("\n" + "=" * 80)
        print("MODE 2: SIDE-BY-SIDE")
        print("=" * 80 + "\n")
        side_by_side_visualization(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 3: WORD-BY-WORD TRACE")
        print("=" * 80 + "\n")
        word_by_word_trace(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 4: LIVE STREAMING")
        print("=" * 80 + "\n")
        live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)


if __name__ == "__main__":
    main()
