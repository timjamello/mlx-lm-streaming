#!/usr/bin/env python3
"""
StreamingLLM Live Visualization Example

This example demonstrates live streaming visualization matching StreamingLLM's
evaluate/streaming_eval.py pattern. It shows:
1. Source text streaming in live as the LLM reads it
2. Target text streaming out underneath as the LLM generates it

The visualization updates both streams in real-time, showing the actual
streaming behavior of the wait-k policy.
"""

import sys

from mlx_lm import load_streaming, stream_generate_streaming_llm


# Long paragraph for translation (matches StreamingLLM examples)
LONG_PARAGRAPH = """
The quick brown fox jumps over the lazy dog. This pangram contains every letter
of the English alphabet at least once. It has been used for testing typewriters
and computer keyboards for many years. The phrase is also commonly used to
demonstrate fonts and typography. In the digital age, it remains a useful tool
for designers and developers who need to see how different characters appear
together. The sentence's simplicity and memorability have made it an enduring
classic in the world of typography and text processing.
"""


def live_streaming_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Live visualization showing input stream and output stream separately.

    This matches StreamingLLM's streaming_eval.py behavior where:
    - Input words appear as they're READ by the model
    - Output words appear as they're GENERATED by the model

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to translate/process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Parse source words for display
    source_words = source_text.strip().split()

    # Print headers
    print("streaming-input:")
    print()  # Space for input stream
    print("streaming-output:")
    print()  # Space for output stream

    # Move cursor up to the input line (ANSI escape: move up 3 lines)
    sys.stdout.write("\033[3A")
    sys.stdout.flush()

    # Track what we've displayed
    last_source_read = 0
    output_words = []

    # Stream generation
    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,  # Greedy for consistency
    ):
        # Update input stream display when new source words are read
        if hasattr(response, "source_words_read") and response.source_words_read > last_source_read:
            # Move cursor to input line
            sys.stdout.write("\033[2A")  # Move up 2 lines to input position
            sys.stdout.write("\r")  # Carriage return to start of line

            # Display source words read so far
            words_to_show = source_words[:response.source_words_read]
            input_display = " ".join(words_to_show)

            # Clear line and print
            sys.stdout.write("\033[K")  # Clear to end of line
            sys.stdout.write(input_display)

            # Move cursor back to output line
            sys.stdout.write("\033[2B")  # Move down 2 lines
            sys.stdout.write("\r")  # Carriage return
            sys.stdout.flush()

            last_source_read = response.source_words_read

        # Update output stream display when word is complete
        if hasattr(response, "word_complete") and response.word_complete:
            # Print the new word on output line
            sys.stdout.write(response.text + " ")
            sys.stdout.flush()
            output_words.append(response.text)

    # Final newline after generation completes
    print()
    print()
    print("=" * 80)
    print(f"Source words read: {last_source_read}")
    print(f"Target words generated: {len(output_words)}")

    if hasattr(response, "generation_tps"):
        print(f"Generation speed: {response.generation_tps:.2f} tokens/sec")
        print(f"Peak memory: {response.peak_memory:.2f} GB")
    print("=" * 80)


def simple_streaming_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Simpler visualization without ANSI cursor control.
    Shows input/output with periodic updates.

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()

    print("streaming-input:")
    displayed_input_words = []

    print("\nstreaming-output:")
    output_words = []

    last_source_read = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,
    ):
        # Show source words as they're read
        if hasattr(response, "source_words_read") and response.source_words_read > last_source_read:
            new_words = source_words[last_source_read:response.source_words_read]
            for word in new_words:
                displayed_input_words.append(word)
            last_source_read = response.source_words_read

        # Show target words as they're generated
        if hasattr(response, "word_complete") and response.word_complete:
            print(response.text, end=" ", flush=True)
            output_words.append(response.text)

    print()
    print()
    print("=" * 80)
    print(f"Input processed: {' '.join(source_words[:last_source_read])}")
    print(f"Source words: {last_source_read}/{len(source_words)}")
    print(f"Target words: {len(output_words)}")
    print("=" * 80)


def side_by_side_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Side-by-side visualization showing source and target word alignment.

    Shows:
    - Which source words have been read
    - Which target words have been generated
    - Current lag between source and target

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"SIDE-BY-SIDE ALIGNMENT VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()

    print(f"{'Source (Read)':<40} | {'Target (Generated)':<40}")
    print("-" * 80)

    source_display = []
    target_display = []

    last_source_read = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,
    ):
        # Update source display
        if hasattr(response, "source_words_read") and response.source_words_read > last_source_read:
            new_words = source_words[last_source_read:response.source_words_read]
            source_display.extend(new_words)
            last_source_read = response.source_words_read

        # Update target display
        if hasattr(response, "word_complete") and response.word_complete:
            target_display.append(response.text)

            # Print current state (truncate if too long)
            source_str = " ".join(source_display)
            target_str = " ".join(target_display)

            # Truncate for display
            if len(source_str) > 38:
                source_str = source_str[:35] + "..."
            if len(target_str) > 38:
                target_str = target_str[:35] + "..."

            # Clear line and print
            sys.stdout.write("\r")
            sys.stdout.write(f"{source_str:<40} | {target_str:<40}")
            sys.stdout.flush()

    print()
    print()
    print("=" * 80)
    print(f"Full source: {' '.join(source_words[:last_source_read])}")
    print()
    print(f"Full target: {' '.join(target_display)}")
    print()
    print(f"Lag: {last_source_read - len(target_display)} words")
    print("=" * 80)


def word_by_word_trace(model, tokenizer, source_text, wait_k=3):
    """
    Detailed word-by-word trace showing streaming progression.

    For each target word generated, shows:
    - Number of source words read at that point
    - Current lag
    - Cumulative output

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"WORD-BY-WORD STREAMING TRACE (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()
    print(f"Source: {source_text}")
    print(f"Source words: {len(source_words)}")
    print()
    print(f"{'Step':<6} {'Target Word':<20} {'Src Read':<10} {'Tgt Gen':<10} {'Lag':<6}")
    print("-" * 80)

    step = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,
    ):
        if hasattr(response, "word_complete") and response.word_complete:
            step += 1
            word = response.text
            src_read = response.source_words_read if hasattr(response, "source_words_read") else "?"
            tgt_gen = response.target_words_generated if hasattr(response, "target_words_generated") else "?"

            if isinstance(src_read, int) and isinstance(tgt_gen, int):
                lag = src_read - tgt_gen
            else:
                lag = "?"

            print(f"{step:<6} {word:<20} {src_read:<10} {tgt_gen:<10} {lag:<6}")

    print()
    print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="StreamingLLM Live Visualization")
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="Model path",
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="live",
        choices=["live", "simple", "side-by-side", "trace", "all"],
        help="Visualization mode",
    )
    parser.add_argument(
        "--wait-k",
        type=int,
        default=3,
        help="Wait-k value",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Custom input prompt (default: long paragraph)",
    )

    args = parser.parse_args()

    # Use long paragraph by default
    if args.prompt is None:
        source_text = LONG_PARAGRAPH.strip()
    else:
        source_text = args.prompt

    print(f"Loading model: {args.model}")
    model, tokenizer = load_streaming(args.model)
    print("Model loaded successfully!\n")

    if args.mode == "live":
        live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "simple":
        simple_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "side-by-side":
        side_by_side_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "trace":
        word_by_word_trace(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "all":
        print("\n" + "=" * 80)
        print("MODE 1: SIMPLE STREAMING")
        print("=" * 80 + "\n")
        simple_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 2: SIDE-BY-SIDE")
        print("=" * 80 + "\n")
        side_by_side_visualization(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 3: WORD-BY-WORD TRACE")
        print("=" * 80 + "\n")
        word_by_word_trace(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 4: LIVE STREAMING")
        print("=" * 80 + "\n")
        live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)


if __name__ == "__main__":
    main()
