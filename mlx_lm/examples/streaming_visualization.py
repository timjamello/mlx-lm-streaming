#!/usr/bin/env python3
"""
StreamingLLM Live Visualization - Juno Conversational Assistant

This example demonstrates a streaming conversational assistant (Juno) that:
1. Continuously processes a stream of diarized speaker segments
2. Uses LLM semantic understanding to determine when to speak
3. Can be interrupted and adapts to the conversation flow
4. Doesn't require wake words - always listening and processing
"""

import os
import sys
import shutil

from mlx_lm import load, stream_generate

# Simulated stream of diarized speaker segments
LONG_PARAGRAPH = """La lumière du matin baignait le petit village d'une douceur paisible ; les volets s'ouvraient lentement, dévoilant des rues pavées encore humides de la rosée. Un chat traversa la place principale, poursuivi par le chant lointain d'un coq, tandis que l'odeur du pain frais s'échappait de la boulangerie. Les habitants, souriants et pressés à la fois, se saluaient en partageant des nouvelles simples, et le temps semblait ralentir pour permettre à chacun d'apprécier ce début de journée serein.
Autour de la fontaine, les premiers rayons du soleil faisaient miroiter des éclats d'or sur la pierre musquée, et quelques pigeons picoraient les miettes laissées par la veille. Une vieille femme, coiffée d'un fichu à motifs fanés, disposait avec soin des pots de fleurs sur son balcon ; les géraniums rouges contrastaient vivement avec le crépi pâle des maisons. Plus loin, le bruit régulier d'une charrette chargée de foin annonçait l'arrivée d'un paysan qui salua d'un signe de tête le boulanger en passant. Les enfants, encore ensommeillés, traînaient des pas traînants en tenant fermement la main d'un parent, leurs cartables balançant doucement au rythme de leurs pas.
L'air, frais et parfumé, portait aussi des notes subtiles de lavande provenant d'un jardin voisin et le parfum âpre du café moulu s'échappait de certaines fenêtres entrebâillées. Sur le trottoir, un café ouvrait ses portes ; deux tables déjà occupées offraient un spectacle vivant : un couple discutait à voix basse, une femme feuilletait le journal tandis qu'un vieil homme sirotait son espresso en observant tranquillement le va-et-vient des passants. Les conversations, ponctuées de rires discrets, racontaient des histoires qui n'avaient d'importance que pour ceux qui les partageaient — la récolte prometteuse dans le champ au bout du chemin, l'arrivée d'un nouveau-né chez le fromager, la réparation prévue du toit de la mairie."""

# System prompt for Juno
JUNO_SYSTEM_PROMPT = """Translate the following from French to English. Do not do anything else. Do not add commentary."""


def live_streaming_visualization(model, tokenizer, source_text, wait_k=50):
    """
    Live visualization showing input stream and output stream separately.

    This matches StreamingLLM's streaming_eval.py behavior where:
    - Input words appear as they're READ by the model
    - Output words appear as they're GENERATED by the model

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to translate/process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Get terminal width for proper wrapping
    try:
        terminal_width = shutil.get_terminal_size().columns
    except:
        terminal_width = 80  # fallback

    # Parse source words for display
    source_words = source_text.strip().split()

    # Track what we've displayed
    last_source_read = 0
    output_words = []

    # Build complete strings for display
    current_input_text = ""
    current_output_text = ""

    # Clear screen and set up initial display
    if os.name == "nt":  # Windows
        os.system("cls")
    else:  # Unix/Linux/Mac
        sys.stdout.write("\033[2J\033[H")  # Clear screen and move to top

    # Print initial headers
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Remember the starting position for our display area
    sys.stdout.write("\033[s")  # Save cursor position

    # Reserve space for the display
    print("conversation-stream (incoming):")
    print()  # Reserve space for input
    print()  # Separator
    print("juno-response (when speaking):")
    print()  # Reserve space for output
    print()  # Extra space

    # Move back to saved position
    sys.stdout.write("\033[u")  # Restore cursor position

    # Stream generation - Juno processing conversation
    for response in stream_generate(
        model=model,
        tokenizer=tokenizer,
        prompt=source_text,  # The diarized speaker stream
        wait_k=wait_k,
        max_new_words=2000,
        system_prompt=JUNO_SYSTEM_PROMPT,
        temp=0.5,  # More natural conversation
        top_p=0.8,
    ):
        # Update displays by completely redrawing the display area
        needs_redraw = False

        # Check if input stream needs update
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            words_to_show = source_words[: response.source_words_read]
            current_input_text = " ".join(words_to_show)
            last_source_read = response.source_words_read
            needs_redraw = True

        # Check if output stream needs update
        if hasattr(response, "word_complete") and response.word_complete:
            output_words.append(response.text)
            current_output_text = "".join(output_words)  # Concatenate without spaces - tokenizer already includes them
            needs_redraw = True

        if needs_redraw:
            # Clear the display area and redraw everything
            sys.stdout.write("\033[u")  # Restore to saved position
            sys.stdout.write("\033[J")  # Clear from cursor to end of screen

            # Redraw the display
            print("conversation-stream (incoming):")
            print(current_input_text if current_input_text else "")
            print()  # Separator
            print("juno-response (when speaking):")
            print(current_output_text if current_output_text else "")

            sys.stdout.flush()

    # Final newline after generation completes
    print()
    print()
    print("=" * 80)
    print(f"Source words read: {last_source_read}")
    print(f"Target words generated: {len(output_words)}")

    if hasattr(response, "generation_tps"):
        print(f"Generation speed: {response.generation_tps:.2f} tokens/sec")
        print(f"Peak memory: {response.peak_memory:.2f} GB")
    print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="StreamingLLM Live Visualization - Juno Conversational Assistant"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="mlx-community/Qwen3-4B-Instruct-2507-4bit",
        help="Model path",
    )
    parser.add_argument(
        "--wait-k",
        type=int,
        default=7,
        help="Wait-k value (number of source words to wait before generating)",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Custom input prompt (default: example conversation)",
    )

    args = parser.parse_args()

    # Use example conversation by default
    if args.prompt is None:
        source_text = LONG_PARAGRAPH.strip()
    else:
        source_text = args.prompt

    print(f"Loading model: {args.model}")
    model, tokenizer = load(args.model)
    print("Model loaded successfully!\n")

    live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)


if __name__ == "__main__":
    main()
