#!/usr/bin/env python3
"""
StreamingLLM Live Visualization Example

This example demonstrates live streaming visualization matching StreamingLLM's
evaluate/streaming_eval.py pattern. It shows:
1. Source text streaming in live as the LLM reads it
2. Target text streaming out underneath as the LLM generates it

The visualization updates both streams in real-time, showing the actual
streaming behavior of the wait-k policy.
"""

import sys

from mlx_lm import load_streaming, stream_generate_streaming_llm

# Long paragraph for translation (matches StreamingLLM examples)
LONG_PARAGRAPH = """
The quick brown fox jumps over the lazy dog. This pangram contains every letter
of the English alphabet at least once. It has been used for testing typewriters
and computer keyboards for many years. The phrase is also commonly used to
demonstrate fonts and typography. In the digital age, it remains a useful tool
for designers and developers who need to see how different characters appear
together. The sentence's simplicity and memorability have made it an enduring
classic in the world of typography and text processing.
"""


def live_streaming_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Live visualization showing input stream and output stream separately.

    This matches StreamingLLM's streaming_eval.py behavior where:
    - Input words appear as they're READ by the model
    - Output words appear as they're GENERATED by the model

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to translate/process
        wait_k: Wait-k policy parameter
    """
    import os
    import shutil

    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Get terminal width for proper wrapping calculation
    try:
        terminal_width = shutil.get_terminal_size().columns
    except:
        terminal_width = 80  # fallback

    # Parse source words for display
    source_words = source_text.strip().split()

    # Track what we've displayed
    last_source_read = 0
    output_words = []

    # Build complete strings for display
    current_input_text = ""
    current_output_text = ""

    # Helper function to calculate number of lines text will occupy
    def calculate_lines(text, width=terminal_width):
        if not text:
            return 1
        # Account for "streaming-input: " or "streaming-output: " prefix on first line
        lines = 0
        current_line_length = 0
        for char in text:
            if char == "\n":
                lines += 1
                current_line_length = 0
            else:
                current_line_length += 1
                if current_line_length >= width:
                    lines += 1
                    current_line_length = 1
        return max(1, lines + (1 if current_line_length > 0 else 0))

    # Clear screen and set up initial display
    if os.name == "nt":  # Windows
        os.system("cls")
    else:  # Unix/Linux/Mac
        sys.stdout.write("\033[2J\033[H")  # Clear screen and move to top

    # Print initial headers
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Remember the starting position for our display area
    sys.stdout.write("\033[s")  # Save cursor position

    # Reserve space for the display
    print("streaming-input:")
    print()  # Reserve multiple lines for input
    print()  # Separator
    print("streaming-output:")
    print()  # Reserve multiple lines for output
    print()  # Extra space

    # Move back to saved position
    sys.stdout.write("\033[u")  # Restore cursor position

    # Stream generation
    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=2000,
        temp=0.3,  # Greedy for consistency
        top_p=0.8,
    ):
        # Update displays by completely redrawing the display area
        needs_redraw = False

        # Check if input stream needs update
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            words_to_show = source_words[: response.source_words_read]
            current_input_text = " ".join(words_to_show)
            last_source_read = response.source_words_read
            needs_redraw = True

        # Check if output stream needs update
        if hasattr(response, "word_complete") and response.word_complete:
            output_words.append(response.text)
            current_output_text = " ".join(output_words)
            needs_redraw = True

        if needs_redraw:
            # Clear the display area and redraw everything
            sys.stdout.write("\033[u")  # Restore to saved position
            sys.stdout.write("\033[J")  # Clear from cursor to end of screen

            # Redraw the display
            print("streaming-input:")
            print(current_input_text if current_input_text else "")
            print()  # Separator
            print("streaming-output:")
            print(current_output_text if current_output_text else "")

            sys.stdout.flush()

    # Final newline after generation completes
    print()
    print()
    print("=" * 80)
    print(f"Source words read: {last_source_read}")
    print(f"Target words generated: {len(output_words)}")

    if hasattr(response, "generation_tps"):
        print(f"Generation speed: {response.generation_tps:.2f} tokens/sec")
        print(f"Peak memory: {response.peak_memory:.2f} GB")
    print("=" * 80)


def simple_streaming_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Simpler visualization without ANSI cursor control.
    Shows input/output with periodic updates.

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()

    print("streaming-input:")
    displayed_input_words = []

    print("\nstreaming-output:")
    output_words = []

    last_source_read = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,
    ):
        # Show source words as they're read
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            new_words = source_words[last_source_read : response.source_words_read]
            for word in new_words:
                displayed_input_words.append(word)
            last_source_read = response.source_words_read

        # Show target words as they're generated
        if hasattr(response, "word_complete") and response.word_complete:
            print(response.text, end=" ", flush=True)
            output_words.append(response.text)

    print()
    print()
    print("=" * 80)
    print(f"Input processed: {' '.join(source_words[:last_source_read])}")
    print(f"Source words: {last_source_read}/{len(source_words)}")
    print(f"Target words: {len(output_words)}")
    print("=" * 80)


def side_by_side_visualization(model, tokenizer, source_text, wait_k=3):
    """
    Side-by-side visualization showing source and target word alignment.

    Shows:
    - Which source words have been read
    - Which target words have been generated
    - Current lag between source and target

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"SIDE-BY-SIDE ALIGNMENT VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()

    print(f"{'Source (Read)':<40} | {'Target (Generated)':<40}")
    print("-" * 80)

    source_display = []
    target_display = []

    last_source_read = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,
    ):
        # Update source display
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            new_words = source_words[last_source_read : response.source_words_read]
            source_display.extend(new_words)
            last_source_read = response.source_words_read

        # Update target display
        if hasattr(response, "word_complete") and response.word_complete:
            target_display.append(response.text)

            # Print current state (truncate if too long)
            source_str = " ".join(source_display)
            target_str = " ".join(target_display)

            # Truncate for display
            if len(source_str) > 38:
                source_str = source_str[:35] + "..."
            if len(target_str) > 38:
                target_str = target_str[:35] + "..."

            # Clear line and print
            sys.stdout.write("\r")
            sys.stdout.write(f"{source_str:<40} | {target_str:<40}")
            sys.stdout.flush()

    print()
    print()
    print("=" * 80)
    print(f"Full source: {' '.join(source_words[:last_source_read])}")
    print()
    print(f"Full target: {' '.join(target_display)}")
    print()
    print(f"Lag: {last_source_read - len(target_display)} words")
    print("=" * 80)


def word_by_word_trace(model, tokenizer, source_text, wait_k=3):
    """
    Detailed word-by-word trace showing streaming progression.

    For each target word generated, shows:
    - Number of source words read at that point
    - Current lag
    - Cumulative output

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"WORD-BY-WORD STREAMING TRACE (wait-k={wait_k})")
    print("=" * 80)
    print()

    source_words = source_text.strip().split()
    print(f"Source: {source_text}")
    print(f"Source words: {len(source_words)}")
    print()
    print(
        f"{'Step':<6} {'Target Word':<20} {'Src Read':<10} {'Tgt Gen':<10} {'Lag':<6}"
    )
    print("-" * 80)

    step = 0

    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=f"Translate the following English paragraph to French: {source_text}",
        wait_k=wait_k,
        max_new_words=200,
        temp=0.0,
    ):
        if hasattr(response, "word_complete") and response.word_complete:
            step += 1
            word = response.text
            src_read = (
                response.source_words_read
                if hasattr(response, "source_words_read")
                else "?"
            )
            tgt_gen = (
                response.target_words_generated
                if hasattr(response, "target_words_generated")
                else "?"
            )

            if isinstance(src_read, int) and isinstance(tgt_gen, int):
                lag = src_read - tgt_gen
            else:
                lag = "?"

            print(f"{step:<6} {word:<20} {src_read:<10} {tgt_gen:<10} {lag:<6}")

    print()
    print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(description="StreamingLLM Live Visualization")
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="Model path",
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="live",
        choices=["live", "simple", "side-by-side", "trace", "all"],
        help="Visualization mode",
    )
    parser.add_argument(
        "--wait-k",
        type=int,
        default=3,
        help="Wait-k value",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Custom input prompt (default: long paragraph)",
    )

    args = parser.parse_args()

    # Use long paragraph by default
    if args.prompt is None:
        source_text = LONG_PARAGRAPH.strip()
    else:
        source_text = args.prompt

    print(f"Loading model: {args.model}")
    model, tokenizer = load_streaming(args.model)
    print("Model loaded successfully!\n")

    if args.mode == "live":
        live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "simple":
        simple_streaming_visualization(
            model, tokenizer, source_text, wait_k=args.wait_k
        )
    elif args.mode == "side-by-side":
        side_by_side_visualization(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "trace":
        word_by_word_trace(model, tokenizer, source_text, wait_k=args.wait_k)
    elif args.mode == "all":
        print("\n" + "=" * 80)
        print("MODE 1: SIMPLE STREAMING")
        print("=" * 80 + "\n")
        simple_streaming_visualization(
            model, tokenizer, source_text, wait_k=args.wait_k
        )

        print("\n" + "=" * 80)
        print("MODE 2: SIDE-BY-SIDE")
        print("=" * 80 + "\n")
        side_by_side_visualization(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 3: WORD-BY-WORD TRACE")
        print("=" * 80 + "\n")
        word_by_word_trace(model, tokenizer, source_text, wait_k=args.wait_k)

        print("\n" + "=" * 80)
        print("MODE 4: LIVE STREAMING")
        print("=" * 80 + "\n")
        live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)


if __name__ == "__main__":
    main()
