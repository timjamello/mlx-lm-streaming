#!/usr/bin/env python3
"""
StreamingLLM Live Visualization - Juno Conversational Assistant

This example demonstrates a streaming conversational assistant (Juno) that:
1. Continuously processes a stream of diarized speaker segments
2. Uses LLM semantic understanding to determine when to speak
3. Can be interrupted and adapts to the conversation flow
4. Doesn't require wake words - always listening and processing
"""

import os
import sys
import shutil

from mlx_lm import load_streaming, stream_generate_streaming_llm

# Simulated stream of diarized speaker segments
LONG_PARAGRAPH = """
[Bob]: Hey Alice, how was your weekend? Did you manage to go to that new restaurant?
[Alice]: Oh, it was great! Yes, we finally went to that Japanese place downtown. The sushi was incredible.
[Bob]: That sounds amazing. I've been meaning to try it. What did you order?
[Alice]: We had the omakase menu. The chef's selection was really impressive.
[Bob]: Juno, can you tell us about omakase dining?
[Alice]: Yes, I'm curious too. What makes it special?
[Bob]: And maybe recommend some dishes we should try if we're new to Japanese cuisine?
[Alice]: That would be helpful. My friend is visiting next week and I'd love to take them somewhere nice.
[Bob]: Juno, also could you suggest some good Japanese restaurants in the downtown area?
[Alice]: Oh, and while you're at it, can you explain the difference between sushi and sashimi?
[Bob]: Right, I always get confused about that.
[Alice]: Me too! And what about wasabi etiquette? I never know how much to use.
[Bob]: Juno, are you there? We'd love your input on Japanese dining.
[Alice]: Maybe she's processing all our questions. We did ask a lot at once!
[Bob]: True. Juno, just start with whatever you think is most important about omakase.
[Alice]: Yes, take your time. We're listening.
"""

# System prompt for Juno
JUNO_SYSTEM_PROMPT = """You are Juno, an AI assistant. You are NOT participating in the conversation—you are OBSERVING a live conversation between other people (like Bob and Alice).

## Critical Rules:
1. You are LISTENING to a conversation, not part of it
2. You will see messages in the format [Speaker]: text
3. You are ONLY "Juno" - you are NOT Bob, NOT Alice, NOT any other speaker
4. DO NOT respond or speak unless someone is clearly addressing you (Juno) and not someone else
5. When processing conversation, note who is speaking, but do NOT speak for them

## When Someone Addresses You:
- Look for phrases that address you by name or are clearly intended for you
- Only THEN should you call the `speak` tool and respond
- Respond as Juno, the AI assistant who has been listening
- Do not echo the conversation while observing - your thoughts should be your own
- Be mindful of interruptions - someone may ask you something, then change the conversation. When this happens, you should stop talking.
- Do not use the speaking tool unless you intend for your response to be spoken
- Do not stop speaking if you have not started speaking

# Tools

You may call one or more functions to assist with responding to the conversation.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"type": "function", "function": {"name": "speak", "description": "Begin speaking (only when addressed!)", "parameters": {"type": "object", "properties": {}, "required": []}}}
{"type": "function", "function": {"name": "stop_speaking", "description": "Stop speaking", "parameters": {"type": "object", "properties": {}, "required": []}}}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>

## Response Protocol:
1. When addressed, call: <tool_call>{"name": "speak", "arguments": {}}</tool_call>
2. Generate your response (ONE complete thought)
3. IMMEDIATELY call: <tool_call>{"name": "stop_speaking", "arguments": {}}</tool_call>
4. DO NOT continue the conversation for others
5. DO NOT generate [Speaker]: tags for other participants

## Prohibited Behaviors:
- ❌ Generating speech for Bob, Alice, or other participants
- ❌ Writing [Bob]: or [Alice]: in your output
- ❌ Continuing past your answer to imagine their responses
- ❌ Forgetting to call stop_speaking

Remember: You are an OBSERVER until addressed."""


def live_streaming_visualization(model, tokenizer, source_text, wait_k=50):
    """
    Live visualization showing input stream and output stream separately.

    This matches StreamingLLM's streaming_eval.py behavior where:
    - Input words appear as they're READ by the model
    - Output words appear as they're GENERATED by the model

    Args:
        model: The language model
        tokenizer: The tokenizer
        source_text: Source text to translate/process
        wait_k: Wait-k policy parameter
    """
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Get terminal width for proper wrapping
    try:
        terminal_width = shutil.get_terminal_size().columns
    except:
        terminal_width = 80  # fallback

    # Parse source words for display
    source_words = source_text.strip().split()

    # Track what we've displayed
    last_source_read = 0
    output_words = []

    # Build complete strings for display
    current_input_text = ""
    current_output_text = ""

    # Clear screen and set up initial display
    if os.name == "nt":  # Windows
        os.system("cls")
    else:  # Unix/Linux/Mac
        sys.stdout.write("\033[2J\033[H")  # Clear screen and move to top

    # Print initial headers
    print("=" * 80)
    print(f"LIVE STREAMING VISUALIZATION (wait-k={wait_k})")
    print("=" * 80)
    print()

    # Remember the starting position for our display area
    sys.stdout.write("\033[s")  # Save cursor position

    # Reserve space for the display
    print("conversation-stream (incoming):")
    print()  # Reserve space for input
    print()  # Separator
    print("juno-response (when speaking):")
    print()  # Reserve space for output
    print()  # Extra space

    # Move back to saved position
    sys.stdout.write("\033[u")  # Restore cursor position

    # Stream generation - Juno processing conversation
    for response in stream_generate_streaming_llm(
        model=model,
        tokenizer=tokenizer,
        prompt=source_text,  # The diarized speaker stream
        wait_k=wait_k,
        max_new_words=2000,
        system_prompt=JUNO_SYSTEM_PROMPT,
        temp=0.5,  # More natural conversation
        top_p=0.8,
    ):
        # Update displays by completely redrawing the display area
        needs_redraw = False

        # Check if input stream needs update
        if (
            hasattr(response, "source_words_read")
            and response.source_words_read > last_source_read
        ):
            words_to_show = source_words[: response.source_words_read]
            current_input_text = " ".join(words_to_show)
            last_source_read = response.source_words_read
            needs_redraw = True

        # Check if output stream needs update
        if hasattr(response, "word_complete") and response.word_complete:
            output_words.append(response.text)
            current_output_text = " ".join(output_words)
            needs_redraw = True

        if needs_redraw:
            # Clear the display area and redraw everything
            sys.stdout.write("\033[u")  # Restore to saved position
            sys.stdout.write("\033[J")  # Clear from cursor to end of screen

            # Redraw the display
            print("conversation-stream (incoming):")
            print(current_input_text if current_input_text else "")
            print()  # Separator
            print("juno-response (when speaking):")
            print(current_output_text if current_output_text else "")

            sys.stdout.flush()

    # Final newline after generation completes
    print()
    print()
    print("=" * 80)
    print(f"Source words read: {last_source_read}")
    print(f"Target words generated: {len(output_words)}")

    if hasattr(response, "generation_tps"):
        print(f"Generation speed: {response.generation_tps:.2f} tokens/sec")
        print(f"Peak memory: {response.peak_memory:.2f} GB")
    print("=" * 80)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="StreamingLLM Live Visualization - Juno Conversational Assistant"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="mlx-community/Qwen3-VL-32B-Instruct-4bit",
        help="Model path",
    )
    parser.add_argument(
        "--wait-k",
        type=int,
        default=7,
        help="Wait-k value (number of source words to wait before generating)",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Custom input prompt (default: example conversation)",
    )

    args = parser.parse_args()

    # Use example conversation by default
    if args.prompt is None:
        source_text = LONG_PARAGRAPH.strip()
    else:
        source_text = args.prompt

    print(f"Loading model: {args.model}")
    model, tokenizer = load_streaming(args.model)
    print("Model loaded successfully!\n")

    live_streaming_visualization(model, tokenizer, source_text, wait_k=args.wait_k)


if __name__ == "__main__":
    main()
